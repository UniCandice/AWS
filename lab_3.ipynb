{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3 (ipykernel)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.11.10"
    },
    "vscode": {
     "interpreter": {
      "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read emr cluster(j-OJTNIKN7EUJ9) details\n",
      "Initiating EMR connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1730825343951_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-224.ec2.internal:20888/proxy/application_1730825343951_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-217.ec2.internal:8042/node/containerlogs/container_1730825343951_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "{\"namespace\": \"sagemaker-analytics\", \"cluster_id\": \"j-OJTNIKN7EUJ9\", \"error_message\": null, \"success\": true, \"service\": \"emr\", \"operation\": \"connect\"}\n"
     ]
    }
   ],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr connect --verify-certificate False --cluster-id j-OJTNIKN7EUJ9 --auth-type None --language python  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Explore and query data from the PySpark kernel\n",
    "\n",
    "In this notebook, you do the following:\n",
    "\n",
    "- Learn how to use an Amazon SageMaker Studio notebook to visually browse, authenticate with, and connect to an Amazon EMR cluster.\n",
    "- Explore and query the data using PySpark. \n",
    "- Monitor your Spark machine learning jobs with the Spark UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Display session information\n",
    "\n",
    "Because you are using the PySpark kernel, you can use the PySpark magic %%info to display the current session information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'name': 'sagemaker_studio_analytics_spark_session_c0fd7c88ba9a4d50888af7c17ade0703', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1730825343951_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-224.ec2.internal:20888/proxy/application_1730825343951_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-217.ec2.internal:8042/node/containerlogs/container_1730825343951_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Explore and query the data\n",
    "\n",
    "When using the PySpark kernel, a SparkContext and a HiveContext are created automatically after connecting to an EMR cluster. You can use HiveContext to query data in the Hive table and make it available in a Spark dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the HiveContext to query Hive and observe the databases and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "| default|adult_data|      false|\n",
      "+--------+----------+-----------+"
     ]
    }
   ],
   "source": [
    "#query-hive\n",
    "\n",
    "sqlContext = HiveContext(sqlContext)\n",
    "\n",
    "dbs = sqlContext.sql(\"show databases\")\n",
    "dbs.show()\n",
    "\n",
    "tables = sqlContext.sql(\"show tables\")\n",
    "tables.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the adult data table and get the data into a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load-data\n",
    "\n",
    "adult_df = sqlContext.sql(\"select * from adult_data\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dataframe to look at the shape and view the first five rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-shape\n",
    "\n",
    "print((adult_df.count(), len(adult_df.columns)))\n",
    "\n",
    "#Show first 5 rows \n",
    "adult_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a cleaner output, convert the Spark dataframe into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert-dataframe\n",
    "\n",
    "adult_df.limit(5).toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some machine learning (ML) algorithms, such as linear regression, require numeric features. The adult dataset that you use in this lab includes categorical features such as **workclass**, **education**, **occupation**, **marital status**, **relationship**, **race**, and **sex**.\n",
    "\n",
    "The following code block illustrates how to use StringIndexer and OneHotEncoderEstimator to convert categorical variables into a set of numeric variables that take on values of 0 and 1.\n",
    "\n",
    "- StringIndexer converts a column of string values to a column of label indexes. \n",
    "- OneHotEncoderEstimator maps a column of category indices to a column of binary vectors, with at most one \"1\" in each row that indicates the category index for that row.\n",
    "\n",
    "One-hot encoding in Spark is a two-step process. You first use the **StringIndexer**, followed by the **OneHotEncoder**.\n",
    "\n",
    "Refer to [StringIndexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer) and [OneHotEncoder](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert-variables\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "categorical_variables = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"-index\") for column in categorical_variables]\n",
    "\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}-encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VectorAssembler class takes multiple columns as the input. It outputs a single column that contains an array of values.\n",
    "\n",
    "Refer to [VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) for more information about this assembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector-assembler\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical-features\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline is an ordered list of transformers and estimators. You can define a pipeline to automate and ensure repeatability of the transformations to be applied to a dataset. In this step, you define the pipeline and then apply it to the dataset.\n",
    "\n",
    "Similar to **StringIndexer**, a pipeline is an **estimator**. The pipeline.fit() method returns a **PipelineModel**, which is a **transformer**.\n",
    "\n",
    "Refer to [Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline) for more information about the machine learning pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-pipelines\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "\n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(adult_df)\n",
    "\n",
    "# Apply the pipeline model to the dataset.\n",
    "adult_df = pipelineModel.transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review all the different columns that were created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print-schema\n",
    "\n",
    "adult_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the transformations, a single column contains an array with every encoded categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view-categorical-features\n",
    "\n",
    "adult_df.select('categorical-features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, encode the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode-target\n",
    "\n",
    "indexer = StringIndexer(inputCol='income', outputCol='label')\n",
    "\n",
    "adult_df = indexer.fit(adult_df).transform(adult_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Monitor and debug with the Spark UI\n",
    "\n",
    "In this section, you use the Spark UI to monitor and inspect the performance of Spark jobs that you ran in the previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the current spark session information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find hyperlinks for the **Spark UI** and **Driver log**. In this lab, the **Driver log** link is inactive. The **Spark UI** presigned URL is generated at the time of connection to the EMR cluster. Choosing this link takes you to the Spark UI to inspect Spark job runs in a web browser. These metrics are helpful for performance tuning. \n",
    "\n",
    "Here are some of the important features to look for in the Spark server:\n",
    "- The **Jobs** tab shows the status of all the Spark jobs in this Spark application.\n",
    "- Under the summary section, the **Event Timeline** section shows the various stages of the run.\n",
    "- The **Completed Jobs** section is shown in a tabular format. Under the **Completed Jobs** section, you can choose a job to review information about the stages of tasks inside it.\n",
    "- Using the **DAG Visualization**, you can explore the tasks that were run earlier. As with the **Event Timeline** view, using the **DAG visualization**, you can choose a stage and expand details within the stage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You successfully:\n",
    "\n",
    "- Learn how to use an Amazon SageMaker Studio notebook to visually browse, authenticate with, and connect to an Amazon EMR cluster.\n",
    "- Explore and query the data using PySpark. \n",
    "- Monitor your Spark machine learning jobs with the Spark UI.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SparkMagic PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Perform data processing with SageMaker Processing\n",
    "\n",
    "In this notebook, you set up the environment needed to run a basic Apache Spark application using Amazon SageMaker Processing. By using Apache Spark on SageMaker Processing, you can run Spark jobs without having to provision an Amazon EMR cluster. You then define and run a Spark job using the **PySparkProcessor** class from the **SageMaker Python SDK**. Finally, you validate the data processing results saved in Amazon Simple Storage Service (Amazon S3).\n",
    "\n",
    "The processing script does some basic data processing, such as string indexing, one-hot encoding, vector assembly, and an 80-20 split of the processed data to train and validate datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Setup the environment\n",
    "\n",
    "Install the latest SageMaker Python SDK package and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install awscli --upgrade\n",
    "%pip install boto3 --upgrade\n",
    "%pip install -U \"sagemaker>2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upgrading the SDK, restart your notebook kernel. \n",
    "\n",
    "1. Choose the **Restart kernel** icon from the notebook toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, import the required libraries, get the execution role to run the SageMaker processing job, and set up the Amazon S3 bucket to store the Spark job outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "SageMaker Execution Role:  arn:aws:iam::281777908394:role/LabVPC-notebook-role\n",
      "Bucket:  labdatabucket-us-west-2-226013955\n"
     ]
    }
   ],
   "source": [
    "#install-dependencies\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "#Execution role to run the SageMaker Processing job\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"SageMaker Execution Role: \", role)\n",
    "\n",
    "#S3 bucket to read the Spark processing script and writing processing job outputs\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error, make sure you restarted your notebook kernel by selecting the **Restart kernel** icon from the notebook toolbar. Then, rerun the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Run the SageMaker processing job\n",
    "\n",
    "In this task, you import and review the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>380922</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>47</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>169699</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>26</td>\n",
       "      <td>Private</td>\n",
       "      <td>248057</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Puerto-Rico</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>45</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>284763</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>500068</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>?</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0           1       2              3   4                    5   \\\n",
       "971  49     Private  380922        Masters  14   Married-civ-spouse   \n",
       "858  47   Local-gov  169699        Masters  14   Married-civ-spouse   \n",
       "324  26     Private  248057        HS-grad   9            Separated   \n",
       "845  45   State-gov  284763   Some-college  10             Divorced   \n",
       "403  27     Private  500068        HS-grad   9   Married-civ-spouse   \n",
       "\n",
       "                     6           7       8        9      10  11  12  \\\n",
       "971     Exec-managerial     Husband   White     Male  15024   0  80   \n",
       "858      Prof-specialty     Husband   White     Male      0   0  40   \n",
       "324   Handlers-cleaners   Own-child   White     Male      0   0  40   \n",
       "845      Prof-specialty   Unmarried   Black   Female      0   0  40   \n",
       "403       Other-service     Husband   White     Male      0   0  36   \n",
       "\n",
       "                 13      14  \n",
       "971   United-States    >50K  \n",
       "858   United-States    >50K  \n",
       "324     Puerto-Rico   <=50K  \n",
       "845   United-States   <=50K  \n",
       "403               ?   <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import-data\n",
    "prefix = 'data/input'\n",
    "\n",
    "S3Downloader.download(s3_uri=f\"s3://{bucket}/{prefix}/spark_adult_data.csv\", local_path= 'data/')\n",
    "\n",
    "shape=pd.read_csv(\"data/spark_adult_data.csv\", header=None)\n",
    "shape.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the SageMaker Spark PySparkProcessor class to define and run a spark application as a processing job. Refer to [SageMaker Spark PySparkProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor) for more information about this class.\n",
    "\n",
    "For creating the PySparkProcessor class, you configure the following parameters:\n",
    "- **base_job_name**: Prefix for the processing job name\n",
    "- **framework_version**: SageMaker PySpark version\n",
    "- **role**: SageMaker execution role\n",
    "- **instance_count**: Number of instances to run the processing job\n",
    "- **instance_type**: Type of Amazon Elastic Compute Cloud (Amazon EC2) instance used for the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark-processor\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# create a PySparkProcessor\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-preprocessor\",\n",
    "    framework_version=\"3.1\", # Spark version\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you use the PySparkProcessor run method to run the **pyspark_preprocessing.py** script as a processing job. Refer to [PySparkProcessor run method](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html#sagemaker.spark.processing.PySparkProcessor.run) for more information about this method. For this lab, data transformations such as string indexing and one-hot encoding are performed on the categorical features.\n",
    "\n",
    "For running the processing job, you configure the following parameters:\n",
    "- **submit_app**: Path of the preprocessing script \n",
    "- **outputs**: Path of output for the preprocessing script (Amazon S3 output locations)\n",
    "- **arguments**: Command-line arguments to the preprocessing script (such as the Amazon S3 input and output locations)\n",
    "\n",
    "The processing job takes approximately 5 minutes to complete. While the job is running, you can review the source for the preprocessing script (which has been preconfigured as part of this lab) by opening the **pyspark_preprocessing.py** file from the file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created S3 bucket: sagemaker-us-west-2-281777908394\n",
      "Creating processing-job with name sm-spark-preprocessor-2024-11-05-14-38-49-477\n",
      "INFO:sagemaker:Creating processing-job with name sm-spark-preprocessor-2024-11-05-14-38-49-477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............\u001b[34m11-05 14:40 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'labdatabucket-us-west-2-226013955', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-226013955', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'labdatabucket-us-west-2-226013955', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-226013955', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1', 'algo-2'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-2', 'algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:281777908394:processing-job/sm-spark-preprocessor-2024-11-05-14-38-49-477', 'ProcessingJobName': 'sm-spark-preprocessor-2024-11-05-14-38-49-477', 'AppSpecification': {'ImageUri': '153931337802.dkr.ecr.us-west-2.amazonaws.com/sagemaker-spark-processing:3.1-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py'], 'ContainerArguments': ['--s3_input_bucket', 'labdatabucket-us-west-2-226013955', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-226013955', '--s3_output_key_prefix', 'data/output']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-west-2-281777908394/sm-spark-preprocessor-2024-11-05-14-38-49-477/input/code/pyspark_preprocessing.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/train', 'S3Uri': 's3://labdatabucket-us-west-2-226013955/data/output/train', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'validation_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/validation', 'S3Uri': 's3://labdatabucket-us-west-2-226013955/data/output/validation', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://labdatabucket-us-west-2-226013955/logs/spark_event_logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::281777908394:role/LabVPC-notebook-role', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/pyspark_preprocessing.py --s3_input_bucket labdatabucket-us-west-2-226013955 --s3_input_key_prefix data/input --s3_output_bucket labdatabucket-us-west-2-226013955 --s3_output_key_prefix data/output\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34m11-05 14:40 waitress     INFO     Serving on http://10.0.232.55:5555\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     Found hadoop jar hadoop-aws-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m11-05 14:40 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[34m11-05 14:40 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m11-05 14:40 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.232.55</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI,AWS_REGION</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\u001b[0m\n",
      "\u001b[34m11-05 14:40 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m11-05 14:40 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.0.232.55\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34m# Fix for \"Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot\u001b[0m\n",
      "\u001b[34m# receive any reply from 10.0.109.30:35219 in 120 seconds.\"\"\u001b[0m\n",
      "\u001b[34mspark.rpc.askTimeout=300s\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 4\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 2\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[34mspark.yarn.appMasterEnv.AWS_REGION us-west-2\u001b[0m\n",
      "\u001b[34mspark.executorEnv.AWS_REGION us-west-2\u001b[0m\n",
      "\u001b[34m11-05 14:40 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m11-05 14:40 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:56,692 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.232.55\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launc\u001b[0m\n",
      "\u001b[34mher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:56,700 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:56,771 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-8a47e930-446c-46f6-90d1-7b3cdbe0477b\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,209 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,220 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,221 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,221 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,225 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,226 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,226 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,226 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,262 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,272 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,272 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,276 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,276 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Nov 05 14:40:57\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,277 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,277 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,278 INFO util.GSet: 2.0% max memory 3.0 GB = 62.2 MB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,278 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,302 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,302 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,308 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,308 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,308 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,308 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,309 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,309 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,309 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,309 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,309 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,309 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,309 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,335 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,336 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,336 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,336 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,347 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,347 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,347 INFO util.GSet: 1.0% max memory 3.0 GB = 31.1 MB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,347 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,360 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,360 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,360 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,360 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,366 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,368 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,371 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,371 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,372 INFO util.GSet: 0.25% max memory 3.0 GB = 7.8 MB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,372 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,378 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,381 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,381 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,383 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,383 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,383 INFO util.GSet: 0.029999999329447746% max memory 3.0 GB = 955.9 KB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,383 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,407 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,419 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,441 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,519 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,526 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,530 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:57,530 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.232.55\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m11-05 14:40 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'labdatabucket-us-west-2-226013955', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-226013955', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/pyspark_preprocessing.py', '--s3_input_bucket', 'labdatabucket-us-west-2-226013955', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-226013955', '--s3_output_key_prefix', 'data/output']\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     {'current_host': 'algo-2', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1', 'algo-2'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-2', 'algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:281777908394:processing-job/sm-spark-preprocessor-2024-11-05-14-38-49-477', 'ProcessingJobName': 'sm-spark-preprocessor-2024-11-05-14-38-49-477', 'AppSpecification': {'ImageUri': '153931337802.dkr.ecr.us-west-2.amazonaws.com/sagemaker-spark-processing:3.1-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/pyspark_preprocessing.py'], 'ContainerArguments': ['--s3_input_bucket', 'labdatabucket-us-west-2-226013955', '--s3_input_key_prefix', 'data/input', '--s3_output_bucket', 'labdatabucket-us-west-2-226013955', '--s3_output_key_prefix', 'data/output']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-west-2-281777908394/sm-spark-preprocessor-2024-11-05-14-38-49-477/input/code/pyspark_preprocessing.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/train', 'S3Uri': 's3://labdatabucket-us-west-2-226013955/data/output/train', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'validation_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/validation', 'S3Uri': 's3://labdatabucket-us-west-2-226013955/data/output/validation', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://labdatabucket-us-west-2-226013955/logs/spark_event_logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::281777908394:role/LabVPC-notebook-role', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/pyspark_preprocessing.py --s3_input_bucket labdatabucket-us-west-2-226013955 --s3_input_key_prefix data/input --s3_output_bucket labdatabucket-us-west-2-226013955 --s3_output_key_prefix data/output\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     Status server listening on algo-2:5555\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[35m11-05 14:40 waitress     INFO     Serving on http://10.0.236.247:5555\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     Found hadoop jar hadoop-aws-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     Optional jar jets3t-0.9.0.jar in /usr/lib/hadoop/lib does not exist\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[35m11-05 14:40 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[35m11-05 14:40 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m11-05 14:40 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[35m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[35m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.232.55</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-2</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-2:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI,AWS_REGION</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[35m</configuration>\u001b[0m\n",
      "\u001b[35m11-05 14:40 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m11-05 14:40 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[35mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.driver.host=10.0.232.55\u001b[0m\n",
      "\u001b[35mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[35m# Fix for \"Uncaught exception: org.apache.spark.rpc.RpcTimeoutException: Cannot\u001b[0m\n",
      "\u001b[35m# receive any reply from 10.0.109.30:35219 in 120 seconds.\"\"\u001b[0m\n",
      "\u001b[35mspark.rpc.askTimeout=300s\u001b[0m\n",
      "\u001b[35mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[35mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[35mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[35mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[35mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[35mspark.executor.cores 4\u001b[0m\n",
      "\u001b[35mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[35mspark.executor.instances 2\u001b[0m\n",
      "\u001b[35mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[35mspark.yarn.appMasterEnv.AWS_REGION us-west-2\u001b[0m\n",
      "\u001b[35mspark.executorEnv.AWS_REGION us-west-2\u001b[0m\n",
      "\u001b[35m11-05 14:40 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[35m11-05 14:40 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/lib/hadoop/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:56,903 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.0.236.247\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launc\u001b[0m\n",
      "\u001b[35mher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:56,918 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:56,920 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.0.236.247\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launc\u001b[0m\n",
      "\u001b[35mher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:56,929 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,484 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,484 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,508 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,562 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,603 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,604 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,605 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,605 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,605 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,606 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,606 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,606 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,609 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,626 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,626 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,674 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,690 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,690 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,765 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,765 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,783 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,805 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,850 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@79e4c792\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,851 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,853 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,853 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,853 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,887 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,893 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,896 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@183ec003\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,896 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,897 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,897 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,897 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,898 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,898 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,902 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.3 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,902 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,925 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,926 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,926 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,927 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,932 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,932 INFO datanode.DataNode: Configured hostname is algo-2\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,932 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,932 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,934 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,935 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,954 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,956 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,956 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:57,959 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,008 INFO util.log: Logging initialized @1696ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,023 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,044 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,201 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,210 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,218 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,220 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,220 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,220 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,250 INFO http.HttpServer2: Jetty bound to port 36527\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,252 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,280 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,280 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,281 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,283 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,283 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,284 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,292 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:33673\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,293 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@463fd068{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,293 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@42b3b079{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,297 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,298 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,306 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,306 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,306 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,308 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,320 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.236.247:33673\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,320 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/10.0.236.247:0\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,320 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,325 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,613 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.232.55\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launc\u001b[0m\n",
      "\u001b[34mher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,633 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,664 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.232.55\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launc\u001b[0m\n",
      "\u001b[34mher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,679 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,711 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.232.55\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launc\u001b[0m\n",
      "\u001b[34mher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.13.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,734 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.232.55\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1-amzn-3\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/accessors-smart-1.2.jar:/usr/lib/hadoop/lib/asm-5.0.4.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-compress-1.18.jar:/usr/lib/hadoop/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop/lib/commons-io-2.5.jar:/usr/lib/hadoop/lib/commons-lang3-3.7.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-net-3.6.jar:/usr/lib/hadoop/lib/commons-text-1.4.jar:/usr/lib/hadoop/lib/curator-client-2.13.0.jar:/usr/lib/hadoop/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jersey-core-1.19.jar:/usr/lib/hadoop/lib/jersey-json-1.19.jar:/usr/lib/hadoop/lib/jersey-server-1.19.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/json-smart-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.25.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-annotations-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-auth-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-fs2img.jar:/usr/lib/hadoop/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kafka.jar:/usr/lib/hadoop/.//hadoop-kms-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/accessors-smart-1.2.jar:/usr/lib/hadoop-hdfs/lib/asm-5.0.4.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.3.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.11.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.18.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.6.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.4.jar:/usr/lib/hadoop-hdfs/lib/curator-client-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-2.13.0.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/gson-2.2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.10.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.20.v20190813.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.54.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/json-smart-2.3.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.5.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.52.Final.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.2.1-amzn-3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.9.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-1.0.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-7.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-fs2img.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-kafka.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-3.2.1-amzn-3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//kafka-clients-2.4.0.jar:/usr/lib/hadoop-mapreduce/.//lz4-java-1.6.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//wildfly-openssl-1.0.7.Final.jar:/usr/lib/hadoop-mapreduce/.//zstd-jni-1.4.3-1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.60.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/lib/hadoop-yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/objenesis-1.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-1.16.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launc\u001b[0m\n",
      "\u001b[34mher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine-3.2.1-amzn-3.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-submarine.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = Unknown -r Unknown; compiled by 'release' on 2021-03-30T23:42Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_332\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,761 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,764 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,356 INFO util.log: Logging initialized @2043ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,363 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,375 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2421cc4{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,383 INFO server.AbstractConnector: Started ServerConnector@518caac3{HTTP/1.1,[http/1.1]}{localhost:36527}\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,384 INFO server.Server: Started @2073ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,473 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,476 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,481 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,483 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,483 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,483 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,484 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,484 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,484 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,486 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,486 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,548 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,553 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,554 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,554 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,599 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,616 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,851 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,851 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,854 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,856 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,878 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,886 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,899 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.232.55:8020 starting to offer service\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,901 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,902 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,904 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,904 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,905 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,918 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,921 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bff64c2{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,922 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3cc41abc{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:58,932 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,033 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[35mNov 05, 2024 2:40:59 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[35mNov 05, 2024 2:40:59 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[35mNov 05, 2024 2:40:59 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[35mNov 05, 2024 2:40:59 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[35mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[35mNov 05, 2024 2:40:59 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mNov 05, 2024 2:40:59 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:58,883 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,163 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,504 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,504 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,582 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://10.0.232.55/\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,641 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,642 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,658 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,768 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,806 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,806 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35mNov 05, 2024 2:40:59 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,611 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d5b5fa7{node,/,file:///tmp/jetty-algo-2-8042-_-any-8152501338116496906.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/node}\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,618 INFO server.AbstractConnector: Started ServerConnector@3f390d63{HTTP/1.1,[http/1.1]}{algo-2:8042}\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,619 INFO server.Server: Started @3305ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,619 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,636 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:33673\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,637 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,643 INFO client.RMProxy: Connecting to ResourceManager at /10.0.232.55:8031\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,696 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,708 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:40:59,978 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[35mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[35m11-05 14:40 smspark-submit INFO     waiting for the primary to come up\u001b[0m\n",
      "\u001b[35m11-05 14:41 smspark-submit INFO     waiting for the primary to go down\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,823 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,824 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,826 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,826 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,828 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,829 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,830 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,832 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,853 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,854 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,917 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,923 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,954 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,959 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,961 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2024-11-05 14:40:59,987 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,012 INFO util.log: Logging initialized @2313ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,038 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,043 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,048 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,093 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,121 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,131 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,132 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,132 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,132 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,177 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,191 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,222 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,227 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,240 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,245 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,251 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@17f9d882\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,253 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,253 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,254 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,255 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,255 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,266 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,267 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,272 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,283 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,285 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,285 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,285 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,314 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,322 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,322 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,328 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,329 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,332 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@7c137fd5\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,332 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,334 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,334 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,334 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,334 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,334 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,334 INFO http.HttpServer2: Jetty bound to port 9870\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,336 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,341 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.3 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,341 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,365 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,366 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,383 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,383 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,385 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,411 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,412 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,415 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,425 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,428 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@302c971f{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,429 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@61c4eee0{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,472 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,473 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,498 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,501 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,502 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,513 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,515 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,515 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,521 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,524 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,535 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,536 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,543 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4a00d9cf{hdfs,/,file:///usr/lib/hadoop-hdfs/webapps/hdfs/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/hdfs}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,547 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,561 INFO server.AbstractConnector: Started ServerConnector@50378a4{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,562 INFO server.Server: Started @2863ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,571 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,578 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,607 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,616 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,632 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,632 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,636 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,673 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,673 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,685 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,685 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,709 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,715 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,718 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,722 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34meffectiveMinResource=<memory:0, vCores:0>\n",
      " , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,722 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,723 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,721 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,724 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,725 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,730 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,731 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,731 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,735 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,737 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,737 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,739 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,750 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,809 INFO util.log: Logging initialized @3114ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,819 INFO util.log: Logging initialized @3100ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,968 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,969 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:00,970 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:00,832 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:00,979 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,041 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,051 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,058 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,062 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,062 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,062 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,063 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,063 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,063 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,065 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,065 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,065 INFO http.HttpServer2: adding path spec: /app/*\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,084 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,084 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,099 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:40505\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,106 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,107 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,110 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,115 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,115 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,126 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,133 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.232.55:40505\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,133 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.232.55:0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,133 WARN tracker.NMLogAggregationStatusTracker: Log Aggregation is disabled.So is the LogAggregationStatusTracker.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,143 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,158 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,172 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,174 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,174 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,180 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,180 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,180 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,181 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,187 INFO util.log: Logging initialized @3475ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,196 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,210 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,236 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,238 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,238 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,239 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,248 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,269 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,270 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,274 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,275 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Nov 05 14:41:01\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,276 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,277 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,278 INFO util.GSet: 2.0% max memory 3.0 GB = 62.2 MB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,282 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,288 INFO http.HttpServer2: Jetty bound to port 34549\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,293 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,326 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,327 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,335 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,335 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,335 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,335 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,336 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,336 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,336 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,336 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,336 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,336 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,336 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,369 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,369 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,370 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,370 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,385 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,385 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,385 INFO util.GSet: 1.0% max memory 3.0 GB = 31.1 MB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,385 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,387 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,388 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,388 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,388 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,389 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,390 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,399 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,403 INFO server.session: node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,423 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,442 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,446 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,446 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,448 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,453 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,453 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,453 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,455 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,456 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,462 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@463fd068{logs,/logs,file:///usr/lib/hadoop/logs/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,464 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@42b3b079{static,/static,file:///usr/lib/hadoop-hdfs/webapps/static/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,467 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,469 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,475 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,483 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,483 INFO util.GSet: 0.25% max memory 3.0 GB = 7.8 MB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,483 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,492 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,498 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,498 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,502 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,503 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,505 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,505 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,505 INFO util.GSet: 0.029999999329447746% max memory 3.0 GB = 955.9 KB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,505 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,532 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 115@algo-1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,571 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,571 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,572 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,606 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,623 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2421cc4{datanode,/,file:///usr/lib/hadoop-hdfs/webapps/datanode/,AVAILABLE}{file:/usr/lib/hadoop-hdfs/webapps/datanode}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,636 INFO server.AbstractConnector: Started ServerConnector@518caac3{HTTP/1.1,[http/1.1]}{localhost:34549}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,637 INFO server.Server: Started @3943ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,658 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,690 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,691 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,703 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,718 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,915 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,939 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,939 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,944 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,997 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:01,997 INFO namenode.FSNamesystem: Finished loading FSImage in 486 msecs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,095 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,099 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,112 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,114 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,128 INFO ipc.Server: Starting Socket Reader #1 for port 9867\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:01,833 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:01,979 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,207 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,207 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,209 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,226 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,236 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,250 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,252 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,335 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@352c1b98{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,341 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d898981{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,354 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,476 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,486 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,496 INFO server.Server: jetty-9.4.20.v20190813; built: 2019-08-13T21:28:18.144Z; git: 84700530e645e812b336747464d6fbbf370c9a20; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,603 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,661 INFO server.session: DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,661 INFO server.session: No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,669 INFO server.session: node0 Scavenging every 660000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,682 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,687 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:02,834 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:02,980 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,699 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,702 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4218500f{logs,/logs,file:///var/log/yarn/,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,704 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@c2db68f{static,/static,jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/static,AVAILABLE}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,740 WARN webapp.WebInfConfiguration: Can't generate resourceBase as part of webapp tmp dir name: java.lang.NullPointerException\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,745 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,749 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,775 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,793 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,808 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.232.55:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,876 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,896 INFO ipc.Server: IPC Server listener on 9867: starting\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:02 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:02 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:02 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:02 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:02,983 INFO util.TypeUtil: JVM Runtime does not support Modules\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:03 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:03 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:03 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:03 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:03 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,195 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,201 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,225 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,353 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,366 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,366 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,367 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:03 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,461 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,504 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,580 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.232.55:8020\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,601 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,616 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,676 INFO namenode.FSDirectory: Quota initialization completed in 60 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,767 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,768 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,768 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,769 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,769 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,769 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:03,769 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 392 msec\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:03 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:03,835 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8031. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,563 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.232.55:8020\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,564 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,569 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 17@algo-2\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,570 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 681504235. Formatting...\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,571 INFO common.Storage: Generated new storageID DS-c1f9e02c-5948-4722-ac34-da865678a5ed for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,591 INFO common.Storage: Analyzing storage directories for bpid BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,591 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,591 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1770039169-10.0.232.55-1730817657400 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,591 INFO common.Storage: Formatting block pool BP-1770039169-10.0.232.55-1730817657400 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,595 INFO datanode.DataNode: Setting up storage: nsid=681504235;bpid=BP-1770039169-10.0.232.55-1730817657400;lv=-57;nsInfo=lv=-65;cid=CID-8a47e930-446c-46f6-90d1-7b3cdbe0477b;nsid=681504235;c=1730817657400;bpid=BP-1770039169-10.0.232.55-1730817657400;dnuuid=null\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,597 INFO datanode.DataNode: Generated and persisted new Datanode UUID 1079ff92-81b1-478b-a6fa-135ccdf0c334\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,676 INFO impl.FsDatasetImpl: Added new volume: DS-c1f9e02c-5948-4722-ac34-da865678a5ed\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,677 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,680 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,687 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,696 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,698 INFO impl.FsDatasetImpl: Adding block pool BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,699 INFO impl.FsDatasetImpl: Scanning block pool BP-1770039169-10.0.232.55-1730817657400 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,732 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1770039169-10.0.232.55-1730817657400 on /opt/amazon/hadoop/hdfs/datanode: 33ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,733 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1770039169-10.0.232.55-1730817657400: 34ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,734 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1770039169-10.0.232.55-1730817657400 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,734 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,735 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1770039169-10.0.232.55-1730817657400 on volume /opt/amazon/hadoop/hdfs/datanode: 1ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,736 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1770039169-10.0.232.55-1730817657400: 2ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,737 INFO datanode.VolumeScanner: Now scanning bpid BP-1770039169-10.0.232.55-1730817657400 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,739 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-c1f9e02c-5948-4722-ac34-da865678a5ed): finished scanning block pool BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,749 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-c1f9e02c-5948-4722-ac34-da865678a5ed): no suitable block pools found to scan.  Waiting 1814399988 ms.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,753 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/5/24 4:58 PM with interval of 21600000ms\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,759 INFO datanode.DataNode: Block pool BP-1770039169-10.0.232.55-1730817657400 (Datanode Uuid 1079ff92-81b1-478b-a6fa-135ccdf0c334) service to algo-1/10.0.232.55:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,835 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,877 INFO datanode.DataNode: Block pool Block pool BP-1770039169-10.0.232.55-1730817657400 (Datanode Uuid 1079ff92-81b1-478b-a6fa-135ccdf0c334) service to algo-1/10.0.232.55:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:04,877 INFO datanode.DataNode: For namenode algo-1/10.0.232.55:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:05,248 INFO datanode.DataNode: Successfully sent block report 0x5ea107ead49e3a79,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 199 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:05,248 INFO datanode.DataNode: Got finalize command for block pool BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:04 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,712 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.232.55:8020\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,722 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,738 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 116@algo-1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,740 INFO common.Storage: Storage directory with location [DISK]file:/opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 681504235. Formatting...\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,742 INFO common.Storage: Generated new storageID DS-d72ca929-dc30-49cf-ae35-f6a0dbd139ed for directory /opt/amazon/hadoop/hdfs/datanode \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,836 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.236.247:9866, datanodeUuid=1079ff92-81b1-478b-a6fa-135ccdf0c334, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-8a47e930-446c-46f6-90d1-7b3cdbe0477b;nsid=681504235;c=1730817657400) storage 1079ff92-81b1-478b-a6fa-135ccdf0c334\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,838 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.236.247:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,838 INFO blockmanagement.BlockReportLeaseManager: Registered DN 1079ff92-81b1-478b-a6fa-135ccdf0c334 (10.0.236.247:9866).\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,871 INFO common.Storage: Analyzing storage directories for bpid BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,879 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,880 INFO common.Storage: Block pool storage directory for location [DISK]file:/opt/amazon/hadoop/hdfs/datanode and block pool id BP-1770039169-10.0.232.55-1730817657400 is not formatted. Formatting ...\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,880 INFO common.Storage: Formatting block pool BP-1770039169-10.0.232.55-1730817657400 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,886 INFO datanode.DataNode: Setting up storage: nsid=681504235;bpid=BP-1770039169-10.0.232.55-1730817657400;lv=-57;nsInfo=lv=-65;cid=CID-8a47e930-446c-46f6-90d1-7b3cdbe0477b;nsid=681504235;c=1730817657400;bpid=BP-1770039169-10.0.232.55-1730817657400;dnuuid=null\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,888 INFO datanode.DataNode: Generated and persisted new Datanode UUID 33c80061-107d-47c3-9915-39f3f0671658\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:04,997 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c1f9e02c-5948-4722-ac34-da865678a5ed for DN 10.0.236.247:9866\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:05 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,073 INFO BlockStateChange: BLOCK* processReport 0x5ea107ead49e3a79: Processing first storage report for DS-c1f9e02c-5948-4722-ac34-da865678a5ed from datanode 1079ff92-81b1-478b-a6fa-135ccdf0c334\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,075 INFO BlockStateChange: BLOCK* processReport 0x5ea107ead49e3a79: from storage DS-c1f9e02c-5948-4722-ac34-da865678a5ed node DatanodeRegistration(10.0.236.247:9866, datanodeUuid=1079ff92-81b1-478b-a6fa-135ccdf0c334, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-8a47e930-446c-46f6-90d1-7b3cdbe0477b;nsid=681504235;c=1730817657400), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,092 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1640c151{node,/,file:///tmp/jetty-algo-1-8042-_-any-4162989034462526646.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/node}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,108 INFO server.AbstractConnector: Started ServerConnector@14dda234{HTTP/1.1,[http/1.1]}{algo-1:8042}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,108 INFO server.Server: Started @7396ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,108 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,132 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:40505\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,133 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,143 INFO client.RMProxy: Connecting to ResourceManager at /10.0.232.55:8031\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,279 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,292 INFO impl.FsDatasetImpl: Added new volume: DS-d72ca929-dc30-49cf-ae35-f6a0dbd139ed\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,293 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/opt/amazon/hadoop/hdfs/datanode, StorageType: DISK\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,297 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,317 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,332 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,363 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,366 INFO impl.FsDatasetImpl: Adding block pool BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[34mNov 05, 2024 2:41:05 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,382 INFO impl.FsDatasetImpl: Scanning block pool BP-1770039169-10.0.232.55-1730817657400 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,464 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1770039169-10.0.232.55-1730817657400 on /opt/amazon/hadoop/hdfs/datanode: 81ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,480 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1770039169-10.0.232.55-1730817657400: 114ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,485 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1770039169-10.0.232.55-1730817657400 on volume /opt/amazon/hadoop/hdfs/datanode...\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,485 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,530 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1770039169-10.0.232.55-1730817657400 on volume /opt/amazon/hadoop/hdfs/datanode: 44ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,530 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1770039169-10.0.232.55-1730817657400: 49ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,538 INFO datanode.VolumeScanner: Now scanning bpid BP-1770039169-10.0.232.55-1730817657400 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,540 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@499683c4{cluster,/,file:///tmp/jetty-10_0_232_55-8088-_-any-2475152805046906659.dir/webapp/,AVAILABLE}{jar:file:/usr/lib/hadoop-yarn/hadoop-yarn-common-3.2.1-amzn-3.jar!/webapps/cluster}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,553 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-d72ca929-dc30-49cf-ae35-f6a0dbd139ed): finished scanning block pool BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,596 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/5/24 7:08 PM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,597 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-d72ca929-dc30-49cf-ae35-f6a0dbd139ed): no suitable block pools found to scan.  Waiting 1814399936 ms.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,629 INFO datanode.DataNode: Block pool BP-1770039169-10.0.232.55-1730817657400 (Datanode Uuid 33c80061-107d-47c3-9915-39f3f0671658) service to algo-1/10.0.232.55:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,644 INFO server.AbstractConnector: Started ServerConnector@51549490{HTTP/1.1,[http/1.1]}{10.0.232.55:8088}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,644 INFO server.Server: Started @7925ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,645 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,706 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.232.55:9866, datanodeUuid=33c80061-107d-47c3-9915-39f3f0671658, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-8a47e930-446c-46f6-90d1-7b3cdbe0477b;nsid=681504235;c=1730817657400) storage 33c80061-107d-47c3-9915-39f3f0671658\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,706 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.232.55:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,706 INFO blockmanagement.BlockReportLeaseManager: Registered DN 33c80061-107d-47c3-9915-39f3f0671658 (10.0.232.55:9866).\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,710 INFO datanode.DataNode: Block pool Block pool BP-1770039169-10.0.232.55-1730817657400 (Datanode Uuid 33c80061-107d-47c3-9915-39f3f0671658) service to algo-1/10.0.232.55:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,710 INFO datanode.DataNode: For namenode algo-1/10.0.232.55:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,775 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,795 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d72ca929-dc30-49cf-ae35-f6a0dbd139ed for DN 10.0.232.55:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,803 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,824 INFO BlockStateChange: BLOCK* processReport 0xcff2e0d057100d9a: Processing first storage report for DS-d72ca929-dc30-49cf-ae35-f6a0dbd139ed from datanode 33c80061-107d-47c3-9915-39f3f0671658\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,824 INFO BlockStateChange: BLOCK* processReport 0xcff2e0d057100d9a: from storage DS-d72ca929-dc30-49cf-ae35-f6a0dbd139ed node DatanodeRegistration(10.0.232.55:9866, datanodeUuid=33c80061-107d-47c3-9915-39f3f0671658, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-8a47e930-446c-46f6-90d1-7b3cdbe0477b;nsid=681504235;c=1730817657400), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,991 INFO datanode.DataNode: Successfully sent block report 0xcff2e0d057100d9a,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 176 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:05,991 INFO datanode.DataNode: Got finalize command for block pool BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,092 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,110 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,104 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,123 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m11-05 14:41 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m11-05 14:41 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m11-05 14:41 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:05,836 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8031. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m11-05 14:41 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,152 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m11-05 14:41 sagemaker-spark-event-logs-publisher INFO     Start to copy the spark event logs file.\u001b[0m\n",
      "\u001b[34m11-05 14:41 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,156 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m11-05 14:41 sagemaker-spark-event-logs-publisher INFO     Writing event log config to spark-defaults.conf\u001b[0m\n",
      "\u001b[34m11-05 14:41 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m11-05 14:41 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2024-11-05T14:41:06.156195')), ('algo-2', StatusMessage(status='WAITING', timestamp='2024-11-05T14:41:06.159751'))])\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,162 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,162 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,163 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,163 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,164 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,165 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,165 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,165 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,194 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,516 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,534 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,535 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,557 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,559 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,588 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,590 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,599 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,602 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,603 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,659 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,671 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,676 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,706 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,740 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,742 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,745 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,929 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,932 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,947 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,952 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,956 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:06,993 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,070 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 40505 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:40505\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,070 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 33673 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:33673\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,075 INFO rmnode.RMNodeImpl: algo-1:40505 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,076 INFO rmnode.RMNodeImpl: algo-2:33673 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,096 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1259087340\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,097 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -565366116\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,098 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:40505 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,111 INFO capacity.CapacityScheduler: Added node algo-1:40505 clusterResource: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:07,113 INFO capacity.CapacityScheduler: Added node algo-2:33673 clusterResource: <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:06,837 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8031. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:07,104 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -1259087340\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:07,105 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -565366116\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:07,105 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:33673 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,720 INFO spark.SparkContext: Running Spark version 3.1.1-amzn-0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,765 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,765 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,766 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,766 INFO spark.SparkContext: Submitted application: PySparkApp\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,789 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 12399, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,804 INFO resource.ResourceProfile: Limiting resource is cpus at 4 tasks per executor\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,806 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,889 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,889 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,889 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,889 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:09,890 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,173 INFO util.Utils: Successfully started service 'sparkDriver' on port 35065.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,199 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,235 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,259 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,259 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,295 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,309 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-98f37c55-ea2a-4763-ad34-abbc92788a76\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,331 INFO memory.MemoryStore: MemoryStore started with capacity 1028.8 MiB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,367 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,471 INFO util.log: Logging initialized @3521ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,554 INFO server.Server: jetty-9.4.37.v20210219; built: 2021-02-19T15:16:47.689Z; git: 27afab2bd37780d179836e313e0fe11bc4fa0ce9; jvm 1.8.0_332-b09\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,577 INFO server.Server: Started @3629ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,615 INFO server.AbstractConnector: Started ServerConnector@a715675{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,615 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,639 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@309c6d75{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,642 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7879deae{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,642 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@780e4f50{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,644 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d2b0d56{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,645 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ceb6928{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,646 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35d09558{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,646 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68c132aa{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,648 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@416954e8{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,649 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2919e82a{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,650 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61f2f9a6{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,650 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f0263f2{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,651 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a5f06bd{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,652 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7bdd417d{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,652 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@42022f69{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,653 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11c7db31{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,653 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@479b2f2d{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,654 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d88c902{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,655 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ec548e0{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,656 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4bf82c62{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,657 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30a45f3f{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,667 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4392d81f{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,668 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10ea26db{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,669 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f7e0ff1{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,669 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@208b2a55{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,670 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d4c79fc{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,672 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.232.55:4040\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:10,951 INFO client.RMProxy: Connecting to ResourceManager at /10.0.232.55:8032\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,217 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,242 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,805 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,806 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,821 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,822 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,822 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,824 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,830 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:11,908 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:17,948 INFO yarn.Client: Uploading resource file:/tmp/spark-64382f3d-cb79-45cd-a344-b3c85b4d0b0e/__spark_libs__8495307604649448711.zip -> hdfs://10.0.232.55/user/root/.sparkStaging/application_1730817666125_0001/__spark_libs__8495307604649448711.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:18,047 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.0.232.55:9866, 10.0.236.247:9866 for /user/root/.sparkStaging/application_1730817666125_0001/__spark_libs__8495307604649448711.zip\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:18,229 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741825_1001 src: /10.0.232.55:34240 dest: /10.0.236.247:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:18,165 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741825_1001 src: /10.0.232.55:47062 dest: /10.0.232.55:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:18,712 INFO DataNode.clienttrace: src: /10.0.232.55:47062, dest: /10.0.232.55:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 33c80061-107d-47c3-9915-39f3f0671658, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741825_1001, duration(ns): 455222873\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:18,712 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.236.247:9866] terminating\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:18,719 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.0.232.55:9866, 10.0.236.247:9866 for /user/root/.sparkStaging/application_1730817666125_0001/__spark_libs__8495307604649448711.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:18,724 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741826_1002 src: /10.0.232.55:47078 dest: /10.0.232.55:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,124 INFO DataNode.clienttrace: src: /10.0.232.55:47078, dest: /10.0.232.55:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 33c80061-107d-47c3-9915-39f3f0671658, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741826_1002, duration(ns): 395870034\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,124 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.236.247:9866] terminating\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,126 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.0.232.55:9866, 10.0.236.247:9866 for /user/root/.sparkStaging/application_1730817666125_0001/__spark_libs__8495307604649448711.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,129 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741827_1003 src: /10.0.232.55:47080 dest: /10.0.232.55:9866\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:18,708 INFO DataNode.clienttrace: src: /10.0.232.55:34240, dest: /10.0.236.247:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 1079ff92-81b1-478b-a6fa-135ccdf0c334, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741825_1001, duration(ns): 456016317\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:18,709 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:18,726 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741826_1002 src: /10.0.232.55:34246 dest: /10.0.236.247:9866\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,122 INFO DataNode.clienttrace: src: /10.0.232.55:34246, dest: /10.0.236.247:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 1079ff92-81b1-478b-a6fa-135ccdf0c334, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741826_1002, duration(ns): 394839909\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,122 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,131 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741827_1003 src: /10.0.232.55:34252 dest: /10.0.236.247:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,448 INFO DataNode.clienttrace: src: /10.0.232.55:47080, dest: /10.0.232.55:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 33c80061-107d-47c3-9915-39f3f0671658, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741827_1003, duration(ns): 314823675\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,448 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.236.247:9866] terminating\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,450 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.0.232.55:9866, 10.0.236.247:9866 for /user/root/.sparkStaging/application_1730817666125_0001/__spark_libs__8495307604649448711.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,453 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741828_1004 src: /10.0.232.55:47096 dest: /10.0.232.55:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,596 INFO DataNode.clienttrace: src: /10.0.232.55:47096, dest: /10.0.232.55:9866, bytes: 46002121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 33c80061-107d-47c3-9915-39f3f0671658, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741828_1004, duration(ns): 139418306\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,596 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.236.247:9866] terminating\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,601 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1730817666125_0001/__spark_libs__8495307604649448711.zip is closed by DFSClient_NONMAPREDUCE_271902488_18\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,705 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.0.232.55/user/root/.sparkStaging/application_1730817666125_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,711 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.0.232.55:9866, 10.0.236.247:9866 for /user/root/.sparkStaging/application_1730817666125_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,714 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741829_1005 src: /10.0.232.55:47098 dest: /10.0.232.55:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,723 INFO DataNode.clienttrace: src: /10.0.232.55:47098, dest: /10.0.232.55:9866, bytes: 889814, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 33c80061-107d-47c3-9915-39f3f0671658, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741829_1005, duration(ns): 5507947\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,723 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.236.247:9866] terminating\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,724 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1730817666125_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_271902488_18\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,733 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9-src.zip -> hdfs://10.0.232.55/user/root/.sparkStaging/application_1730817666125_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,741 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.0.232.55:9866, 10.0.236.247:9866 for /user/root/.sparkStaging/application_1730817666125_0001/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,743 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741830_1006 src: /10.0.232.55:47108 dest: /10.0.232.55:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,748 INFO DataNode.clienttrace: src: /10.0.232.55:47108, dest: /10.0.232.55:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 33c80061-107d-47c3-9915-39f3f0671658, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741830_1006, duration(ns): 2285954\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,748 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.236.247:9866] terminating\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,750 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1730817666125_0001/py4j-0.10.9-src.zip is closed by DFSClient_NONMAPREDUCE_271902488_18\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,867 INFO yarn.Client: Uploading resource file:/tmp/spark-64382f3d-cb79-45cd-a344-b3c85b4d0b0e/__spark_conf__774395045542264469.zip -> hdfs://10.0.232.55/user/root/.sparkStaging/application_1730817666125_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,875 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.0.232.55:9866, 10.0.236.247:9866 for /user/root/.sparkStaging/application_1730817666125_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,878 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741831_1007 src: /10.0.232.55:47114 dest: /10.0.232.55:9866\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,885 INFO DataNode.clienttrace: src: /10.0.232.55:47114, dest: /10.0.232.55:9866, bytes: 266122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 33c80061-107d-47c3-9915-39f3f0671658, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741831_1007, duration(ns): 3042192\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,885 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.236.247:9866] terminating\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,888 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1730817666125_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_271902488_18\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,908 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,908 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,908 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,908 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,908 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:19,930 INFO yarn.Client: Submitting application application_1730817666125_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,011 INFO capacity.CapacityScheduler: Application 'application_1730817666125_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,011 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,031 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,032 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,033 INFO rmapp.RMAppImpl: Storing application with id application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,034 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.232.55#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,040 INFO recovery.RMStateStore: Storing info for app: application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,040 INFO rmapp.RMAppImpl: application_1730817666125_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,041 INFO rmapp.RMAppImpl: application_1730817666125_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,042 INFO capacity.ParentQueue: Application added - appId: application_1730817666125_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,043 INFO capacity.CapacityScheduler: Accepted application application_1730817666125_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,053 INFO rmapp.RMAppImpl: application_1730817666125_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,082 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,082 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,156 INFO impl.YarnClientImpl: Submitted application application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,162 INFO capacity.LeafQueue: Application application_1730817666125_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,163 INFO capacity.LeafQueue: Application added - appId: application_1730817666125_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,163 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1730817666125_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,446 INFO DataNode.clienttrace: src: /10.0.232.55:34252, dest: /10.0.236.247:9866, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 1079ff92-81b1-478b-a6fa-135ccdf0c334, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741827_1003, duration(ns): 313674372\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,447 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,455 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741828_1004 src: /10.0.232.55:34264 dest: /10.0.236.247:9866\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,595 INFO DataNode.clienttrace: src: /10.0.232.55:34264, dest: /10.0.236.247:9866, bytes: 46002121, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 1079ff92-81b1-478b-a6fa-135ccdf0c334, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741828_1004, duration(ns): 138391087\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,595 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,716 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741829_1005 src: /10.0.232.55:34280 dest: /10.0.236.247:9866\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,722 INFO DataNode.clienttrace: src: /10.0.232.55:34280, dest: /10.0.236.247:9866, bytes: 889814, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 1079ff92-81b1-478b-a6fa-135ccdf0c334, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741829_1005, duration(ns): 4697425\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,722 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,744 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741830_1006 src: /10.0.232.55:34286 dest: /10.0.236.247:9866\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,747 INFO DataNode.clienttrace: src: /10.0.232.55:34286, dest: /10.0.236.247:9866, bytes: 41587, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 1079ff92-81b1-478b-a6fa-135ccdf0c334, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741830_1006, duration(ns): 1503077\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,747 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,880 INFO datanode.DataNode: Receiving BP-1770039169-10.0.232.55-1730817657400:blk_1073741831_1007 src: /10.0.232.55:34292 dest: /10.0.236.247:9866\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,884 INFO DataNode.clienttrace: src: /10.0.232.55:34292, dest: /10.0.236.247:9866, bytes: 266122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_271902488_18, offset: 0, srvID: 1079ff92-81b1-478b-a6fa-135ccdf0c334, blockid: BP-1770039169-10.0.232.55-1730817657400:blk_1073741831_1007, duration(ns): 2066224\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:19,884 INFO datanode.DataNode: PacketResponder: BP-1770039169-10.0.232.55-1730817657400:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,170 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,222 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1730817666125_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,225 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,226 INFO fica.FiCaSchedulerNode: Assigned container container_1730817666125_0001_01_000001 of capacity <memory:896, max memory:15892, vCores:1, max vCores:4> on host algo-2:33673, which has 1 containers, <memory:896, vCores:1> used and <memory:14996, vCores:3> available after allocation\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,226 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,245 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:33673 for container : container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,252 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,252 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,253 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,253 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1730817666125_0001 AttemptId: appattempt_1730817666125_0001_000001 MasterContainer: Container: [ContainerId: container_1730817666125_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:33673, NodeHttpAddress: algo-2:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.236.247:33673 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,253 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,263 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,267 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,281 INFO amlauncher.AMLauncher: Launching masterappattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,332 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1730817666125_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:33673, NodeHttpAddress: algo-2:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.236.247:33673 }, ExecutionType: GUARANTEED, ] for AM appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,333 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,336 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,620 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1730817666125_0001_01_000001, AllocationRequestId: -1, Version: 0, NodeId: algo-2:33673, NodeHttpAddress: algo-2:8042, Resource: <memory:896, max memory:15892, vCores:1, max vCores:4>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.236.247:33673 }, ExecutionType: GUARANTEED, ] for AM appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,621 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,621 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1730817666125_0001, attemptId: appattempt_1730817666125_0001_000001launchTime: 1730817680620\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:20,621 INFO recovery.RMStateStore: Updating info for app: application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:21,160 INFO yarn.Client: Application report for application_1730817666125_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:21,163 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1730817680031\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1730817666125_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,454 INFO ipc.Server: Auth successful for appattempt_1730817666125_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,542 INFO containermanager.ContainerManagerImpl: Start request for container_1730817666125_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,595 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,605 INFO application.ApplicationImpl: Application application_1730817666125_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,605 INFO application.ApplicationImpl: Adding container_1730817666125_0001_01_000001 to application application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,605 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.232.55#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,610 INFO application.ApplicationImpl: Application application_1730817666125_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,614 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,614 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,625 INFO localizer.ResourceLocalizationService: Created localizer for container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,702 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1730817666125_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,718 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,723 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1730817666125_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:20,723 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:21,218 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:22,165 INFO yarn.Client: Application report for application_1730817666125_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:23,168 INFO yarn.Client: Application report for application_1730817666125_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:24,032 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:24,033 INFO scheduler.ContainerScheduler: Starting container [container_1730817666125_0001_01_000001]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:24,061 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:24,062 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:24,067 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/directory.info\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:24,170 INFO yarn.Client: Application report for application_1730817666125_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:25,323 INFO monitor.ContainersMonitorImpl: container_1730817666125_0001_01_000001's ip = 10.0.236.247, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:25,335 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1730817666125_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:25,174 INFO yarn.Client: Application report for application_1730817666125_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,034 INFO ipc.Server: Auth successful for appattempt_1730817666125_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,054 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,055 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.236.247#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011APPATTEMPTID=appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,055 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,055 INFO rmapp.RMAppImpl: application_1730817666125_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m11-05 14:41 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,180 INFO yarn.Client: Application report for application_1730817666125_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,181 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.236.247\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1730817680031\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1730817666125_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,182 INFO cluster.YarnClientSchedulerBackend: Application application_1730817666125_0001 has started running.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,199 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34299.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,199 INFO netty.NettyBlockTransferService: Server created on 10.0.232.55:34299\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,202 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,216 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.232.55, 34299, None)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,220 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.232.55:34299 with 1028.8 MiB RAM, BlockManagerId(driver, 10.0.232.55, 34299, None)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,227 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.232.55, 34299, None)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,230 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.232.55, 34299, None)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,319 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1730817666125_0001), /proxy/application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,451 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,454 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6da4e41c{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,487 INFO history.SingleEventLogFileWriter: Logging events to file:/tmp/spark-events/application_1730817666125_0001.inprogress\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,725 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,928 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,928 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,942 INFO ui.ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,943 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d8ae009{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,943 INFO ui.ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,944 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@556a13ad{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,945 INFO ui.ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,945 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7158c5c7{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,945 INFO ui.ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,946 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a31a00d{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,947 INFO ui.ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:26,948 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c9d5bfc{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,038 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,040 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,040 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,417 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,417 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,418 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,418 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,419 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,525 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,671 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,898 INFO client.RMProxy: Connecting to ResourceManager at /10.0.232.55:8030\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:25,952 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:26,172 INFO client.TransportClientFactory: Successfully created connection to /10.0.232.55:35065 after 66 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:26,354 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:27,050 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] Default YARN executor launch context:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     AWS_REGION -> us-west-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.0.232.55/user/root/.sparkStaging/application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       -Xmx12399m \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-Dspark.driver.port=35065' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       '-Dspark.rpc.askTimeout=300s' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.232.55:35065 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:27,226 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:27,800 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:27,810 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:27,811 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       4 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       application_1730817666125_0001 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       --resourceProfileId \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       0 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.0.232.55\" port: -1 file: \"/user/root/.sparkStaging/application_1730817666125_0001/pyspark.zip\" } size: 889814 timestamp: 1730817679724 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.0.232.55\" port: -1 file: \"/user/root/.sparkStaging/application_1730817666125_0001/__spark_libs__8495307604649448711.zip\" } size: 448655305 timestamp: 1730817679601 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     py4j-0.10.9-src.zip -> resource { scheme: \"hdfs\" host: \"10.0.232.55\" port: -1 file: \"/user/root/.sparkStaging/application_1730817666125_0001/py4j-0.10.9-src.zip\" } size: 41587 timestamp: 1730817679750 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.0.232.55\" port: -1 file: \"/user/root/.sparkStaging/application_1730817666125_0001/__spark_conf__.zip\" } size: 266122 timestamp: 1730817679888 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:27,123 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 1239, script: , vendor: , cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 12399, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2024-11-05 14:41:27,124 INFO yarn.YarnAllocator: Resource profile 0 doesn't exist, adding it\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,221 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1730817666125_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,222 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,222 INFO fica.FiCaSchedulerNode: Assigned container container_1730817666125_0001_01_000002 of capacity <memory:13638, vCores:1> on host algo-1:40505, which has 1 containers, <memory:13638, vCores:1> used and <memory:2254, vCores:3> available after allocation\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,222 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,223 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.4572741 absoluteUsedCapacity=0.4572741 used=<memory:14534, vCores:2> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,223 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,235 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1730817666125_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,236 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000003 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,236 INFO fica.FiCaSchedulerNode: Assigned container container_1730817666125_0001_01_000003 of capacity <memory:13638, vCores:1> on host algo-2:33673, which has 2 containers, <memory:14534, vCores:2> used and <memory:1358, vCores:2> available after allocation\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,236 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,237 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8863579 absoluteUsedCapacity=0.8863579 used=<memory:28172, vCores:3> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,237 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,902 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:40505 for container : container_1730817666125_0001_01_000002\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,903 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,904 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:33673 for container : container_1730817666125_0001_01_000003\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:28,905 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,134 INFO ipc.Server: Auth successful for appattempt_1730817666125_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,162 INFO datasources.InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,036 INFO ipc.Server: Auth successful for appattempt_1730817666125_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,045 INFO containermanager.ContainerManagerImpl: Start request for container_1730817666125_0001_01_000003 by user root\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,047 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.236.247#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000003\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,047 INFO application.ApplicationImpl: Adding container_1730817666125_0001_01_000003 to application application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,048 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000003 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,048 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,049 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000003 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,049 INFO scheduler.ContainerScheduler: Starting container [container_1730817666125_0001_01_000003]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,068 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000003 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,069 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1730817666125_0001_01_000003\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:29,072 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000003/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000001/stderr] 2Handling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/launch_container.sh\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,233 INFO containermanager.ContainerManagerImpl: Start request for container_1730817666125_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,238 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,305 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,316 INFO application.ApplicationImpl: Application application_1730817666125_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,316 INFO application.ApplicationImpl: Adding container_1730817666125_0001_01_000002 to application application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,317 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.236.247#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000002\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,322 INFO application.ApplicationImpl: Application application_1730817666125_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,332 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,332 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,344 INFO localizer.ResourceLocalizationService: Created localizer for container_1730817666125_0001_01_000002\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,461 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1730817666125_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,501 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,507 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1730817666125_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:29,507 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524378   56 -r-x------   1 root     root        55236 Nov  5 14:41 ./__spark_libs__/geronimo-jcache_1.0_spec-1.0-alpha-1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524405  900 -r-x------   1 root     root       919285 Nov  5 14:41 ./__spark_libs__/hive-serde-2.3.7-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524476   64 -r-x------   1 root     root        65464 Nov  5 14:41 ./__spark_libs__/kerb-common-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524586  504 -r-x------   1 root     root       512742 Nov  5 14:41 ./__spark_libs__/woodstox-core-5.0.3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524386   44 -r-x------   1 root     root        44168 Nov  5 14:41 ./__spark_libs__/hadoop-client-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524361  268 -r-x------   1 root     root       273370 Nov  5 14:41 ./__spark_libs__/commons-net-3.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524534  708 -r-x------   1 root     root       723203 Nov  5 14:41 ./__spark_libs__/parquet-format-2.4.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524393  316 -r-x------   1 root     root       323162 Nov  5 14:41 ./__spark_libs__/hadoop-yarn-client-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524383   76 -r-x------   1 root     root        76983 Nov  5 14:41 ./__spark_libs__/guice-servlet-4.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524321   32 -r-x------   1 root     root        30035 Nov  5 14:41 ./__spark_libs__/accessors-smart-1.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524359   64 -r-x------   1 root     root        62050 Nov  5 14:41 ./__spark_libs__/commons-logging-1.1.3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524353  164 -r-x------   1 root     root       166244 Nov  5 14:41 ./__spark_libs__/commons-crypto-1.1.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524376   20 -r-x------   1 root     root        18497 Nov  5 14:41 ./__spark_libs__/flatbuffers-java-1.9.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524557  532 -r-x------   1 root     root       542288 Nov  5 14:41 ./__spark_libs__/spark-hive-thriftserver_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524451  240 -r-x------   1 root     root       244502 Nov  5 14:41 ./__spark_libs__/jersey-client-2.30.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524365  164 -r-x------   1 root     root       164422 Nov  5 14:41 ./__spark_libs__/core-1.1.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524537  524 -r-x------   1 root     root       533455 Nov  5 14:41 ./__spark_libs__/protobuf-java-2.5.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524442    4 -r-x------   1 root     root         2497 Nov  5 14:41 ./__spark_libs__/javax.inject-1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524490  308 -r-x------   1 root     root       313702 Nov  5 14:41 ./__spark_libs__/libfb303-0.9.3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524416 1468 -r-x------   1 root     root      1502280 Nov  5 14:41 ./__spark_libs__/htrace-core4-4.1.0-incubating.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524489 1024 -r-x------   1 root     root      1045744 Nov  5 14:41 ./__spark_libs__/leveldbjni-all-1.8.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524344   60 -r-x------   1 root     root        58684 Nov  5 14:41 ./__spark_libs__/chill-java-0.9.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524445  388 -r-x------   1 root     root       395195 Nov  5 14:41 ./__spark_libs__/javolution-5.5.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524426  112 -r-x------   1 root     root       111007 Nov  5 14:41 ./__spark_libs__/jackson-datatype-jsr310-2.11.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524507 1712 -r-x------   1 root     root      1749371 Nov  5 14:41 ./__spark_libs__/netlib-native_ref-linux-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524497  104 -r-x------   1 root     root       105365 Nov  5 14:41 ./__spark_libs__/metrics-core-4.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524531 1076 -r-x------   1 root     root      1098935 Nov  5 14:41 ./__spark_libs__/parquet-column-1.10.1-spark-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524575   36 -r-x------   1 root     root        34601 Nov  5 14:41 ./__spark_libs__/spire-util_2.12-0.17.0-M1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524467  504 -r-x------   1 root     root       512151 Nov  5 14:41 ./__spark_libs__/json4s-core_2.12-3.7.0-M5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524430   36 -r-x------   1 root     root        34998 Nov  5 14:41 ./__spark_libs__/jackson-module-jaxb-annotations-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524521  324 -r-x------   1 root     root       331034 Nov  5 14:41 ./__spark_libs__/okhttp-2.7.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524397   80 -r-x------   1 root     root        81406 Nov  5 14:41 ./__spark_libs__/hadoop-yarn-server-web-proxy-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524382  656 -r-x------   1 root     root       668235 Nov  5 14:41 ./__spark_libs__/guice-4.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524596  160 -r-x------   1 root     root       159755 Nov  5 14:41 ./__spark_libs__/aws-glue-datacatalog-hive3-client.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524379   44 -r-x------   1 root     root        44513 Nov  5 14:41 ./__spark_libs__/gmetric4j-1.0.10.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524420 1256 -r-x------   1 root     root      1282424 Nov  5 14:41 ./__spark_libs__/ivy-2.4.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524360 1988 -r-x------   1 root     root      2035066 Nov  5 14:41 ./__spark_libs__/commons-math3-3.4.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524536 1024 -r-x------   1 root     root      1048529 Nov  5 14:41 ./__spark_libs__/parquet-jackson-1.10.1-spark-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524316  136 -r-x------   1 root     root       136363 Nov  5 14:41 ./__spark_libs__/HikariCP-2.5.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524333 1672 -r-x------   1 root     root      1711185 Nov  5 14:41 ./__spark_libs__/arrow-vector-2.0.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524483  100 -r-x------   1 root     root       102174 Nov  5 14:41 ./__spark_libs__/kerby-asn1-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524318 1152 -r-x------   1 root     root      1175798 Nov  5 14:41 ./__spark_libs__/JTransforms-3.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524525  796 -r-x------   1 root     root       814000 Nov  5 14:41 ./__spark_libs__/orc-core-1.5.12.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524387 4080 -r-x------   1 root     root      4175105 Nov  5 14:41 ./__spark_libs__/hadoop-common-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524501   24 -r-x------   1 root     root        23909 Nov  5 14:41 ./__spark_libs__/metrics-jvm-4.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524368  280 -r-x------   1 root     root       283653 Nov  5 14:41 ./__spark_libs__/curator-recipes-2.13.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524438  140 -r-x------   1 root     root       140376 Nov  5 14:41 ./__spark_libs__/jakarta.ws.rs-api-2.1.6.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524373  304 -r-x------   1 root     root       307637 Nov  5 14:41 ./__spark_libs__/dnsjava-2.1.7.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524319  380 -r-x------   1 root     root       386529 Nov  5 14:41 ./__spark_libs__/RoaringBitmap-0.9.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524462  420 -r-x------   1 root     root       427780 Nov  5 14:41 ./__spark_libs__/jodd-core-3.5.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524372 3152 -r-x------   1 root     root      3224708 Nov  5 14:41 ./__spark_libs__/derby-10.12.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524488  404 -r-x------   1 root     root       410874 Nov  5 14:41 ./__spark_libs__/kryo-shaded-4.0.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524473    8 -r-x------   1 root     root         4592 Nov  5 14:41 ./__spark_libs__/jul-to-slf4j-1.7.30.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524385  136 -r-x------   1 root     root       139121 Nov  5 14:41 ./__spark_libs__/hadoop-auth-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524546  544 -r-x------   1 root     root       556575 Nov  5 14:41 ./__spark_libs__/scala-xml_2.12-1.2.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524381 2140 -r-x------   1 root     root      2189117 Nov  5 14:41 ./__spark_libs__/guava-14.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524524   20 -r-x------   1 root     root        19827 Nov  5 14:41 ./__spark_libs__/opencsv-2.3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524455   76 -r-x------   1 root     root        76733 Nov  5 14:41 ./__spark_libs__/jersey-hk2-2.30.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524520   56 -r-x------   1 root     root        55684 Nov  5 14:41 ./__spark_libs__/objenesis-2.6.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524317  228 -r-x------   1 root     root       232470 Nov  5 14:41 ./__spark_libs__/JLargeArrays-1.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524406 1644 -r-x------   1 root     root      1679364 Nov  5 14:41 ./__spark_libs__/hive-service-rpc-3.1.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524472   16 -r-x------   1 root     root        15071 Nov  5 14:41 ./__spark_libs__/jta-1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524556  424 -r-x------   1 root     root       431100 Nov  5 14:41 ./__spark_libs__/spark-graphx_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524558  688 -r-x------   1 root     root       703130 Nov  5 14:41 ./__spark_libs__/spark-hive_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524348  280 -r-x------   1 root     root       284184 Nov  5 14:41 ./__spark_libs__/commons-codec-1.10.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524528   64 -r-x------   1 root     root        65261 Nov  5 14:41 ./__spark_libs__/oro-2.0.8.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524457  908 -r-x------   1 root     root       927721 Nov  5 14:41 ./__spark_libs__/jersey-server-2.30.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524417  760 -r-x------   1 root     root       774384 Nov  5 14:41 ./__spark_libs__/httpclient-4.5.9.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524512  304 -r-x------   1 root     root       310891 Nov  5 14:41 ./__spark_libs__/netlib-native_system-linux-armhf-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524570   16 -r-x------   1 root     root        15171 Nov  5 14:41 ./__spark_libs__/spark-tags_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524388 5008 -r-x------   1 root     root      5127896 Nov  5 14:41 ./__spark_libs__/hadoop-hdfs-client-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524325  164 -r-x------   1 root     root       167761 Nov  5 14:41 ./__spark_libs__/antlr-runtime-3.5.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524584  440 -r-x------   1 root     root       447005 Nov  5 14:41 ./__spark_libs__/univocity-parsers-2.9.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524349  576 -r-x------   1 root     root       588337 Nov  5 14:41 ./__spark_libs__/commons-collections-3.2.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524593  492 -r-x------   1 root     root       501704 Nov  5 14:41 ./__spark_libs__/hadoop-aws-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524478  116 -r-x------   1 root     root       116120 Nov  5 14:41 ./__spark_libs__/kerb-crypto-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524542 10424 -r-x------   1 root     root     10672015 Nov  5 14:41 ./__spark_libs__/scala-compiler-2.12.10.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524555   28 -r-x------   1 root     root        28313 Nov  5 14:41 ./__spark_libs__/spark-ganglia-lgpl_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524320  232 -r-x------   1 root     root       236660 Nov  5 14:41 ./__spark_libs__/ST4-4.0.4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524427   36 -r-x------   1 root     root        33205 Nov  5 14:41 ./__spark_libs__/jackson-jaxrs-base-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524559   64 -r-x------   1 root     root        61835 Nov  5 14:41 ./__spark_libs__/spark-kvstore_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524465  120 -r-x------   1 root     root       120316 Nov  5 14:41 ./__spark_libs__/json-smart-2.3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524550   12 -r-x------   1 root     root        12211 Nov  5 14:41 ./__spark_libs__/slf4j-log4j12-1.7.30.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524439  116 -r-x------   1 root     root       115498 Nov  5 14:41 ./__spark_libs__/jakarta.xml.bind-api-2.3.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524404 8084 -r-x------   1 root     root      8274798 Nov  5 14:41 ./__spark_libs__/hive-metastore-2.3.7-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524588  100 -r-x------   1 root     root        99555 Nov  5 14:41 ./__spark_libs__/xz-1.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524502    8 -r-x------   1 root     root         5711 Nov  5 14:41 ./__spark_libs__/minlog-1.3.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524538  124 -r-x------   1 root     root       123052 Nov  5 14:41 ./__spark_libs__/py4j-0.10.9.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524576 7020 -r-x------   1 root     root      7188024 Nov  5 14:41 ./__spark_libs__/spire_2.12-0.17.0-M1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524535  284 -r-x------   1 root     root       290168 Nov  5 14:41 ./__spark_libs__/parquet-hadoop-1.10.1-spark-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524428   16 -r-x------   1 root     root        15562 Nov  5 14:41 ./__spark_libs__/jackson-jaxrs-json-provider-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524434   28 -r-x------   1 root     root        25058 Nov  5 14:41 ./__spark_libs__/jakarta.annotation-api-1.3.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524347   44 -r-x------   1 root     root        41123 Nov  5 14:41 ./__spark_libs__/commons-cli-1.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524563 2348 -r-x------   1 root     root      2400610 Nov  5 14:41 ./__spark_libs__/spark-network-common_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524500   20 -r-x------   1 root     root        16642 Nov  5 14:41 ./__spark_libs__/metrics-json-4.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524441  764 -r-x------   1 root     root       780265 Nov  5 14:41 ./__spark_libs__/javassist-3.25.0-GA.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524506 1456 -r-x------   1 root     root      1489507 Nov  5 14:41 ./__spark_libs__/netlib-native_ref-linux-i686-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524527   28 -r-x------   1 root     root        27755 Nov  5 14:41 ./__spark_libs__/orc-shims-1.5.12.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524522  420 -r-x------   1 root     root       427674 Nov  5 14:41 ./__spark_libs__/okhttp-3.12.12.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524345  208 -r-x------   1 root     root       211523 Nov  5 14:41 ./__spark_libs__/chill_2.12-0.9.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524518 4120 -r-x------   1 root     root      4216895 Nov  5 14:41 ./__spark_libs__/netty-all-4.1.51.Final.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524513  428 -r-x------   1 root     root       435365 Nov  5 14:41 ./__spark_libs__/netlib-native_system-linux-i686-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524471   36 -r-x------   1 root     root        33031 Nov  5 14:41 ./__spark_libs__/jsr305-3.0.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524390 1632 -r-x------   1 root     root      1669989 Nov  5 14:41 ./__spark_libs__/hadoop-mapreduce-client-core-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524334  176 -r-x------   1 root     root       176285 Nov  5 14:41 ./__spark_libs__/automaton-1.11-8.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524357  280 -r-x------   1 root     root       284220 Nov  5 14:41 ./__spark_libs__/commons-lang-2.6.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524480   84 -r-x------   1 root     root        82756 Nov  5 14:41 ./__spark_libs__/kerb-server-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524425   48 -r-x------   1 root     root        46781 Nov  5 14:41 ./__spark_libs__/jackson-dataformat-yaml-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524343 3152 -r-x------   1 root     root      3226851 Nov  5 14:41 ./__spark_libs__/cats-kernel_2.12-2.0.0-M4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524339 4092 -r-x------   1 root     root      4189874 Nov  5 14:41 ./__spark_libs__/bcprov-jdk15on-1.60.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524423  228 -r-x------   1 root     root       232248 Nov  5 14:41 ./__spark_libs__/jackson-core-asl-1.9.13.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524503   56 -r-x------   1 root     root        54461 Nov  5 14:41 ./__spark_libs__/native_ref-java-1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524419   28 -r-x------   1 root     root        27156 Nov  5 14:41 ./__spark_libs__/istack-commons-runtime-3.0.8.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524498   24 -r-x------   1 root     root        22042 Nov  5 14:41 ./__spark_libs__/metrics-graphite-4.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524578  160 -r-x------   1 root     root       161867 Nov  5 14:41 ./__spark_libs__/stax2-api-3.1.4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524597  152 -r-x------   1 root     root       153385 Nov  5 14:41 ./__spark_libs__/aws-glue-datacatalog-spark-client-3.2.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524324 1144 -r-x------   1 root     root      1168113 Nov  5 14:41 ./__spark_libs__/algebra_2.12-2.0.0-M2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524332   40 -r-x------   1 root     root        39931 Nov  5 14:41 ./__spark_libs__/arrow-memory-netty-2.0.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524443  244 -r-x------   1 root     root       249790 Nov  5 14:41 ./__spark_libs__/javax.jdo-3.2.0-m3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524336  132 -r-x------   1 root     root       132989 Nov  5 14:41 ./__spark_libs__/avro-ipc-1.8.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524468   36 -r-x------   1 root     root        35943 Nov  5 14:41 ./__spark_libs__/json4s-jackson_2.12-3.7.0-M5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524351  620 -r-x------   1 root     root       632424 Nov  5 14:41 ./__spark_libs__/commons-compress-1.20.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524568 1116 -r-x------   1 root     root      1141204 Nov  5 14:41 ./__spark_libs__/spark-streaming_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524581  232 -r-x------   1 root     root       233745 Nov  5 14:41 ./__spark_libs__/threeten-extra-1.5.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524585  384 -r-x------   1 root     root       392124 Nov  5 14:41 ./__spark_libs__/velocity-1.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524371 1864 -r-x------   1 root     root      1908681 Nov  5 14:41 ./__spark_libs__/datanucleus-rdbms-4.1.19.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524431   44 -r-x------   1 root     root        43743 Nov  5 14:41 ./__spark_libs__/jackson-module-paranamer-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524587  276 -r-x------   1 root     root       281356 Nov  5 14:41 ./__spark_libs__/xbean-asm7-shaded-4.15.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524322   68 -r-x------   1 root     root        69409 Nov  5 14:41 ./__spark_libs__/activation-1.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524532   96 -r-x------   1 root     root        95171 Nov  5 14:41 ./__spark_libs__/parquet-common-1.10.1-spark-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524508 1800 -r-x------   1 root     root      1839499 Nov  5 14:41 ./__spark_libs__/netlib-native_ref-osx-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524496    4 -r-x------   1 root     root         3180 Nov  5 14:41 ./__spark_libs__/macro-compat_2.12-1.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524577   28 -r-x------   1 root     root        26514 Nov  5 14:41 ./__spark_libs__/stax-api-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524561  116 -r-x------   1 root     root       115371 Nov  5 14:41 ./__spark_libs__/spark-mllib-local_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524487   32 -r-x------   1 root     root        29134 Nov  5 14:41 ./__spark_libs__/kerby-xdr-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524395  224 -r-x------   1 root     root       226000 Nov  5 14:41 ./__spark_libs__/hadoop-yarn-registry-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524453   32 -r-x------   1 root     root        32091 Nov  5 14:41 ./__spark_libs__/jersey-container-servlet-2.30.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524517  624 -r-x------   1 root     root       637428 Nov  5 14:41 ./__spark_libs__/netlib-native_system-win-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524391   84 -r-x------   1 root     root        85904 Nov  5 14:41 ./__spark_libs__/hadoop-mapreduce-client-jobclient-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524516  552 -r-x------   1 root     root       561459 Nov  5 14:41 ./__spark_libs__/netlib-native_system-win-i686-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524440  908 -r-x------   1 root     root       926574 Nov  5 14:41 ./__spark_libs__/janino-3.0.16.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524475  112 -r-x------   1 root     root       113017 Nov  5 14:41 ./__spark_libs__/kerb-client-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524504   56 -r-x------   1 root     root        54509 Nov  5 14:41 ./__spark_libs__/native_system-java-1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524466   84 -r-x------   1 root     root        85865 Nov  5 14:41 ./__spark_libs__/json4s-ast_2.12-3.7.0-M5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524401 10640 -r-x------   1 root     root     10892246 Nov  5 14:41 ./__spark_libs__/hive-exec-2.3.7-amzn-4-core.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524328   28 -r-x------   1 root     root        27006 Nov  5 14:41 ./__spark_libs__/aopalliance-repackaged-2.6.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524529   20 -r-x------   1 root     root        19479 Nov  5 14:41 ./__spark_libs__/osgi-resource-locator-1.0.3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524548    4 -r-x------   1 root     root         2500 Nov  5 14:41 ./__spark_libs__/shims-0.9.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524408   12 -r-x------   1 root     root        10486 Nov  5 14:41 ./__spark_libs__/hive-shims-2.3.7-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524429  764 -r-x------   1 root     root       780664 Nov  5 14:41 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524486   40 -r-x------   1 root     root        40554 Nov  5 14:41 ./__spark_libs__/kerby-util-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524459  264 -r-x------   1 root     root       268780 Nov  5 14:41 ./__spark_libs__/jline-2.14.6.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524407   56 -r-x------   1 root     root        56951 Nov  5 14:41 ./__spark_libs__/hive-shims-0.23-2.3.7-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524543 5156 -r-x------   1 root     root      5276900 Nov  5 14:41 ./__spark_libs__/scala-library-2.12.10.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524435   20 -r-x------   1 root     root        18140 Nov  5 14:41 ./__spark_libs__/jakarta.inject-2.6.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524394 2864 -r-x------   1 root     root      2929005 Nov  5 14:41 ./__spark_libs__/hadoop-yarn-common-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524374   16 -r-x------   1 root     root        15935 Nov  5 14:41 ./__spark_libs__/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524544  220 -r-x------   1 root     root       222980 Nov  5 14:41 ./__spark_libs__/scala-parser-combinators_2.12-1.1.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524533  832 -r-x------   1 root     root       848783 Nov  5 14:41 ./__spark_libs__/parquet-encoding-1.10.1-spark-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524479   20 -r-x------   1 root     root        20046 Nov  5 14:41 ./__spark_libs__/kerb-identity-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524355  160 -r-x------   1 root     root       160519 Nov  5 14:41 ./__spark_libs__/commons-dbcp-1.4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524565   52 -r-x------   1 root     root        52877 Nov  5 14:41 ./__spark_libs__/spark-repl_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524323  132 -r-x------   1 root     root       134044 Nov  5 14:41 ./__spark_libs__/aircompressor-0.10.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524492  480 -r-x------   1 root     root       489884 Nov  5 14:41 ./__spark_libs__/log4j-1.2.17.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524564  128 -r-x------   1 root     root       127582 Nov  5 14:41 ./__spark_libs__/spark-network-shuffle_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524491  244 -r-x------   1 root     root       246445 Nov  5 14:41 ./__spark_libs__/libthrift-0.12.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524369  360 -r-x------   1 root     root       366748 Nov  5 14:41 ./__spark_libs__/datanucleus-api-jdo-4.2.4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524470  100 -r-x------   1 root     root       100636 Nov  5 14:41 ./__spark_libs__/jsp-api-2.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524409  120 -r-x------   1 root     root       121048 Nov  5 14:41 ./__spark_libs__/hive-shims-common-2.3.7-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524389  788 -r-x------   1 root     root       805850 Nov  5 14:41 ./__spark_libs__/hadoop-mapreduce-client-common-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524580   92 -r-x------   1 root     root        93210 Nov  5 14:41 ./__spark_libs__/super-csv-2.2.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524509 1784 -r-x------   1 root     root      1826785 Nov  5 14:41 ./__spark_libs__/netlib-native_ref-win-i686-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524362   96 -r-x------   1 root     root        96221 Nov  5 14:41 ./__spark_libs__/commons-pool-1.5.4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524530   36 -r-x------   1 root     root        34654 Nov  5 14:41 ./__spark_libs__/paranamer-2.8.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524415  132 -r-x------   1 root     root       131590 Nov  5 14:41 ./__spark_libs__/hk2-utils-2.6.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524574   12 -r-x------   1 root     root         8261 Nov  5 14:41 ./__spark_libs__/spire-platform_2.12-0.17.0-M1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524433   44 -r-x------   1 root     root        44399 Nov  5 14:41 ./__spark_libs__/jakarta.activation-api-1.2.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524418  320 -r-x------   1 root     root       326874 Nov  5 14:41 ./__spark_libs__/httpcore-4.4.11.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524330   72 -r-x------   1 root     root        72668 Nov  5 14:41 ./__spark_libs__/arrow-format-2.0.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524481   20 -r-x------   1 root     root        20409 Nov  5 14:41 ./__spark_libs__/kerb-simplekdc-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524364   80 -r-x------   1 root     root        79845 Nov  5 14:41 ./__spark_libs__/compress-lzf-1.0.3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524335 1524 -r-x------   1 root     root      1556863 Nov  5 14:41 ./__spark_libs__/avro-1.8.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524554 10504 -r-x------   1 root     root     10752238 Nov  5 14:41 ./__spark_libs__/spark-core_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524342 13504 -r-x------   1 root     root     13826799 Nov  5 14:41 ./__spark_libs__/breeze_2.12-1.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524540  128 -r-x------   1 root     root       127223 Nov  5 14:41 ./__spark_libs__/remotetea-oncrpc-1.1.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524572  356 -r-x------   1 root     root       363074 Nov  5 14:41 ./__spark_libs__/spark-yarn_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524477  224 -r-x------   1 root     root       226672 Nov  5 14:41 ./__spark_libs__/kerb-core-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524377   16 -r-x------   1 root     root        14395 Nov  5 14:41 ./__spark_libs__/generex-1.0.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524566   32 -r-x------   1 root     root        30517 Nov  5 14:41 ./__spark_libs__/spark-sketch_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524493   16 -r-x------   1 root     root        12483 Nov  5 14:41 ./__spark_libs__/logging-interceptor-3.12.12.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524432  336 -r-x------   1 root     root       342143 Nov  5 14:41 ./__spark_libs__/jackson-module-scala_2.12-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524392 3216 -r-x------   1 root     root      3290504 Nov  5 14:41 ./__spark_libs__/hadoop-yarn-api-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524424 1376 -r-x------   1 root     root      1404939 Nov  5 14:41 ./__spark_libs__/jackson-databind-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524447  992 -r-x------   1 root     root      1013367 Nov  5 14:41 ./__spark_libs__/jaxb-runtime-2.3.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524380  188 -r-x------   1 root     root       190432 Nov  5 14:41 ./__spark_libs__/gson-2.2.4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524414  200 -r-x------   1 root     root       203358 Nov  5 14:41 ./__spark_libs__/hk2-locator-2.6.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524474   80 -r-x------   1 root     root        80980 Nov  5 14:41 ./__spark_libs__/kerb-admin-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524338  780 -r-x------   1 root     root       796532 Nov  5 14:41 ./__spark_libs__/bcpkix-jdk15on-1.60.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524354   24 -r-x------   1 root     root        24239 Nov  5 14:41 ./__spark_libs__/commons-daemon-1.0.13.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524422  344 -r-x------   1 root     root       349331 Nov  5 14:41 ./__spark_libs__/jackson-core-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524346  244 -r-x------   1 root     root       246918 Nov  5 14:41 ./__spark_libs__/commons-beanutils-1.9.4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524350   72 -r-x------   1 root     root        71626 Nov  5 14:41 ./__spark_libs__/commons-compiler-3.0.16.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524598  152 -r-x------   1 root     root       153385 Nov  5 14:41 ./__spark_libs__/aws-glue-datacatalog-spark-client.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524567 7948 -r-x------   1 root     root      8135275 Nov  5 14:41 ./__spark_libs__/spark-sql_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524327    8 -r-x------   1 root     root         4467 Nov  5 14:41 ./__spark_libs__/aopalliance-1.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524582   20 -r-x------   1 root     root        18763 Nov  5 14:41 ./__spark_libs__/token-provider-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524562 6012 -r-x------   1 root     root      6154474 Nov  5 14:41 ./__spark_libs__/spark-mllib_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524358  512 -r-x------   1 root     root       523372 Nov  5 14:41 ./__spark_libs__/commons-lang3-3.10.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524547 3168 -r-x------   1 root     root      3243337 Nov  5 14:41 ./__spark_libs__/shapeless_2.12-2.3.3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524461  628 -r-x------   1 root     root       643043 Nov  5 14:41 ./__spark_libs__/joda-time-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524573   80 -r-x------   1 root     root        79588 Nov  5 14:41 ./__spark_libs__/spire-macros_2.12-0.17.0-M1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524437   92 -r-x------   1 root     root        91930 Nov  5 14:41 ./__spark_libs__/jakarta.validation-api-2.0.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524485  200 -r-x------   1 root     root       204650 Nov  5 14:41 ./__spark_libs__/kerby-pkix-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524539  100 -r-x------   1 root     root       100431 Nov  5 14:41 ./__spark_libs__/pyrolite-4.30.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524326  332 -r-x------   1 root     root       337869 Nov  5 14:41 ./__spark_libs__/antlr4-runtime-4.8-1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524399   48 -r-x------   1 root     root        46219 Nov  5 14:41 ./__spark_libs__/hive-cli-2.3.7-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524458   36 -r-x------   1 root     root        34392 Nov  5 14:41 ./__spark_libs__/jetty-rewrite-9.3.27.v20190418.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524590 1480 -r-x------   1 root     root      1515244 Nov  5 14:41 ./__spark_libs__/zookeeper-3.4.14.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524552 1924 -r-x------   1 root     root      1969177 Nov  5 14:41 ./__spark_libs__/snappy-java-1.1.8.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524460   12 -r-x------   1 root     root         8685 Nov  5 14:41 ./__spark_libs__/jniloader-1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524583   16 -r-x------   1 root     root        15071 Nov  5 14:41 ./__spark_libs__/transaction-api-1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524421   68 -r-x------   1 root     root        68080 Nov  5 14:41 ./__spark_libs__/jackson-annotations-2.10.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524329 1168 -r-x------   1 root     root      1194003 Nov  5 14:41 ./__spark_libs__/arpack_combined_all-0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524352  604 -r-x------   1 root     root       616888 Nov  5 14:41 ./__spark_libs__/commons-configuration2-2.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524375 1688 -r-x------   1 root     root      1726527 Nov  5 14:41 ./__spark_libs__/ehcache-3.3.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524519  296 -r-x------   1 root     root       299508 Nov  5 14:41 ./__spark_libs__/nimbus-jose-jwt-4.41.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524444   96 -r-x------   1 root     root        95806 Nov  5 14:41 ./__spark_libs__/javax.servlet-api-3.1.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524579  176 -r-x------   1 root     root       178149 Nov  5 14:41 ./__spark_libs__/stream-2.9.6.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524484   32 -r-x------   1 root     root        30674 Nov  5 14:41 ./__spark_libs__/kerby-config-1.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524494  636 -r-x------   1 root     root       649950 Nov  5 14:41 ./__spark_libs__/lz4-java-1.7.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524450  200 -r-x------   1 root     root       201124 Nov  5 14:41 ./__spark_libs__/jdo-api-3.0.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524356  204 -r-x------   1 root     root       208700 Nov  5 14:41 ./__spark_libs__/commons-io-2.5.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524553 10008 -r-x------   1 root     root     10247828 Nov  5 14:41 ./__spark_libs__/spark-catalyst_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524505 1180 -r-x------   1 root     root      1208080 Nov  5 14:41 ./__spark_libs__/netlib-native_ref-linux-armhf-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524411  232 -r-x------   1 root     root       236672 Nov  5 14:41 ./__spark_libs__/hive-storage-api-2.7.2.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524410   16 -r-x------   1 root     root        13942 Nov  5 14:41 ./__spark_libs__/hive-shims-scheduler-2.3.7-amzn-4.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524413  196 -r-x------   1 root     root       200223 Nov  5 14:41 ./__spark_libs__/hk2-api-2.6.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524514  448 -r-x------   1 root     root       458605 Nov  5 14:41 ./__spark_libs__/netlib-native_system-linux-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524396 1364 -r-x------   1 root     root      1393617 Nov  5 14:41 ./__spark_libs__/hadoop-yarn-server-common-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524384   60 -r-x------   1 root     root        60263 Nov  5 14:41 ./__spark_libs__/hadoop-annotations-3.2.1-amzn-3.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524499   24 -r-x------   1 root     root        20889 Nov  5 14:41 ./__spark_libs__/metrics-jmx-4.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524541  112 -r-x------   1 root     root       112235 Nov  5 14:41 ./__spark_libs__/scala-collection-compat_2.12-2.1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524331  100 -r-x------   1 root     root       100990 Nov  5 14:41 ./__spark_libs__/arrow-memory-core-2.0.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524367  200 -r-x------   1 root     root       201965 Nov  5 14:41 ./__spark_libs__/curator-framework-2.13.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524591 6260 -r-x------   1 root     root      6407352 Nov  5 14:41 ./__spark_libs__/zstd-jni-1.4.8-1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524515  544 -r-x------   1 root     root       553993 Nov  5 14:41 ./__spark_libs__/netlib-native_system-osx-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524448    8 -r-x------   1 root     root         4722 Nov  5 14:41 ./__spark_libs__/jcip-annotations-1.0-1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524523   84 -r-x------   1 root     root        85756 Nov  5 14:41 ./__spark_libs__/okio-1.14.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524592 211800 -r-x------   1 root     root     216879203 Nov  5 14:41 ./__spark_libs__/aws-java-sdk-bundle-1.11.977.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524370 1972 -r-x------   1 root     root      2016766 Nov  5 14:41 ./__spark_libs__/datanucleus-core-4.1.17.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524495   36 -r-x------   1 root     root        33786 Nov  5 14:41 ./__spark_libs__/machinist_2.12-0.6.8.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524511  128 -r-x------   1 root     root       128414 Nov  5 14:41 ./__spark_libs__/re2j-1.1.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524594  116 -r-x------   1 root     root       116694 Nov  5 14:41 ./__spark_libs__/aws-glue-datacatalog-client-common-3.2.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524510 2252 -r-x------   1 root     root      2305169 Nov  5 14:41 ./__spark_libs__/netlib-native_ref-win-x86_64-1.1-natives.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524452 1140 -r-x------   1 root     root      1166647 Nov  5 14:41 ./__spark_libs__/jersey-common-2.30.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524589   36 -r-x------   1 root     root        35518 Nov  5 14:41 ./__spark_libs__/zjsonpatch-0.3.0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524560   76 -r-x------   1 root     root        76928 Nov  5 14:41 ./__spark_libs__/spark-launcher_2.12-3.1.1-amzn-0.jar\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524482   36 -r-x------   1 root     root        36708 Nov  5 14:41 ./__spark_libs__/kerb-util-1.0.1.jar\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:30,244 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:31,358 INFO monitor.ContainersMonitorImpl: container_1730817666125_0001_01_000003's ip = 10.0.236.247, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:31,363 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1730817666125_0001_01_000003 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:31,729 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.236.247:51752) with ID 2,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:31,923 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:41431 with 6.3 GiB RAM, BlockManagerId(2, algo-2, 41431, None)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:31,932 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/directory.info] 524526   48 -r-x------   1 root     root        4820[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,053 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 508@algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,062 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,063 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,064 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,538 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,538 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,539 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,540 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,540 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:30,936 INFO client.TransportClientFactory: Successfully created connection to /10.0.232.55:35065 after 77 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,104 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,104 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,104 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,104 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,104 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,155 INFO client.TransportClientFactory: Successfully created connection to /10.0.232.55:35065 after 2 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,226 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/blockmgr-0323c695-165d-4cc1-91c6-c2d2f53406dc\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,271 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GiB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,183 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,184 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,186 INFO datasources.FileSourceStrategy: Output Data Schema: struct<workclass: string>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,648 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 317.5 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,713 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,717 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,721 INFO spark.SparkContext: Created broadcast 0 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,806 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,814 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,886 INFO scheduler.DAGScheduler: Registering RDD 3 (collect at StringIndexer.scala:204) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,905 INFO scheduler.DAGScheduler: Got map stage job 0 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,905 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,906 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,907 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:32,916 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,159 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 19.4 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,165 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,166 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.232.55:34299 (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,167 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,193 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,194 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,241 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,536 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:41431 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,960 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,961 INFO scheduler.ContainerScheduler: Starting container [container_1730817666125_0001_01_000002]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,989 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,990 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1730817666125_0001_01_000002\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:33,994 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/directory.info\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:34,140 INFO monitor.ContainersMonitorImpl: container_1730817666125_0001_01_000002's ip = 10.0.232.55, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:34,149 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1730817666125_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,560 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.232.55:35065\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,584 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,585 INFO resource.ResourceUtils: No custom resources configured for spark.executor.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,585 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,737 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,743 INFO executor.Executor: Starting executor ID 2 on host algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,896 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41431.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,897 INFO netty.NettyBlockTransferService: Server created on algo-2:41431\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,899 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,913 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-2, 41431, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,927 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-2, 41431, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,928 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, algo-2, 41431, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,948 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,973 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:31,973 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:32,932 INFO executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 1264 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:33,284 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:33,295 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:33,418 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:34,686 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:41431 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:36,533 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.232.55:37858) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:36,719 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:42047 with 6.3 GiB RAM, BlockManagerId(1, algo-1, 42047, None)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:36,977 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3748 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:36,980 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,014 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at StringIndexer.scala:204) finished in 4.049 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,020 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,021 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,021 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,021 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:34,946 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1225@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:34,953 INFO util.SignalUtils: Registering signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:34,953 INFO util.SignalUtils: Registering signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:34,954 INFO util.SignalUtils: Registering signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,461 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,462 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,462 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,463 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,463 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,859 INFO client.TransportClientFactory: Successfully created connection to /10.0.232.55:35065 after 68 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,959 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,959 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,959 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,959 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:35,960 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,012 INFO client.TransportClientFactory: Successfully created connection to /10.0.232.55:35065 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,068 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/blockmgr-d050450e-644d-409e-894b-268fe043fe2f\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,111 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GiB\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,256 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,260 INFO scheduler.DAGScheduler: Got job 1 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,260 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,261 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,262 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,268 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,295 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.0 KiB, free 1028.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,298 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,303 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.232.55:34299 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,305 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,307 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,308 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,359 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (algo-2, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,410 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:41431 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,410 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:41431 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,436 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.232.55:34299 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,615 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.236.247:51752\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,804 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 456 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,804 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,806 INFO scheduler.DAGScheduler: ResultStage 2 (collect at StringIndexer.scala:204) finished in 0.526 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,810 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,810 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:37,812 INFO scheduler.DAGScheduler: Job 1 finished: collect at StringIndexer.scala:204, took 0.555225 s\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:33,477 INFO client.TransportClientFactory: Successfully created connection to /10.0.232.55:34299 after 2 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:33,529 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:33,542 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 123 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:33,641 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:34,248 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:34,672 INFO codegen.CodeGenerator: Code generated in 202.797017 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:34,674 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:34,684 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:34,688 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 13 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:34,748 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:35,376 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:35,422 INFO codegen.CodeGenerator: Code generated in 9.521142 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:35,441 INFO codegen.CodeGenerator: Code generated in 6.529956 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:35,455 INFO codegen.CodeGenerator: Code generated in 8.612965 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:36,520 INFO codegen.CodeGenerator: Code generated in 14.390311 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:36,669 INFO codegen.CodeGenerator: Code generated in 14.415265 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:36,947 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2119 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,367 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,367 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,196 INFO codegen.CodeGenerator: Code generated in 265.290503 ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,711 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,711 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,711 INFO datasources.FileSourceStrategy: Output Data Schema: struct<education: string>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,740 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 317.5 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,752 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,753 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,754 INFO spark.SparkContext: Created broadcast 3 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,755 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,756 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,761 INFO scheduler.DAGScheduler: Registering RDD 10 (collect at StringIndexer.scala:204) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,762 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,762 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,762 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,763 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,763 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,775 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.4 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,787 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,788 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.232.55:34299 (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,789 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,790 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,790 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,792 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,808 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:41431 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:38,826 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:41431 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,015 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 224 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,016 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,017 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (collect at StringIndexer.scala:204) finished in 0.251 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,018 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,018 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,018 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,018 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,045 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,047 INFO scheduler.DAGScheduler: Got job 3 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,047 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,047 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,047 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,048 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[13] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,052 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.0 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,053 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,054 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.232.55:34299 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,055 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,055 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,055 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,057 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (algo-2, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,070 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:41431 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,077 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.236.247:51752\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,113 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 57 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,114 INFO scheduler.DAGScheduler: ResultStage 5 (collect at StringIndexer.scala:204) finished in 0.065 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,115 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,117 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,117 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,117 INFO scheduler.DAGScheduler: Job 3 finished: collect at StringIndexer.scala:204, took 0.071582 s\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,375 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,376 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,402 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,440 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 64 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,442 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,605 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,607 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.232.55:35065)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,664 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,687 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,688 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,727 INFO codegen.CodeGenerator: Code generated in 14.4548 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:37,778 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 1). 3530 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,794 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,795 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 2)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,797 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,803 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,808 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,810 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,816 INFO datasources.FileScanRDD: TID: 2 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,819 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,824 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,826 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,866 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:38,955 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,012 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 2). 2119 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,060 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,060 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 3)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,062 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,063 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,068 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,071 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,072 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,076 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,076 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.232.55:35065)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,078 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,079 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,234 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,234 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,234 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marital_status: string>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,257 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,270 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,271 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,272 INFO spark.SparkContext: Created broadcast 6 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,273 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,274 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,278 INFO scheduler.DAGScheduler: Registering RDD 17 (collect at StringIndexer.scala:204) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,278 INFO scheduler.DAGScheduler: Got map stage job 4 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,278 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,278 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,278 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,280 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,287 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.4 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,289 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,289 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.232.55:34299 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,289 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,290 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,290 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,291 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,302 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:41431 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,315 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:41431 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,414 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 123 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,414 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,415 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at StringIndexer.scala:204) finished in 0.134 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,416 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,416 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,416 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,416 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,444 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,449 INFO scheduler.DAGScheduler: Got job 5 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,449 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,449 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,449 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,453 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[20] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,456 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,458 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,458 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.232.55:34299 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,459 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,460 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,460 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,461 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (algo-2, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,473 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:41431 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,480 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.236.247:51752\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,512 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 51 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,512 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,514 INFO scheduler.DAGScheduler: ResultStage 8 (collect at StringIndexer.scala:204) finished in 0.060 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,515 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,515 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,516 INFO scheduler.DAGScheduler: Job 5 finished: collect at StringIndexer.scala:204, took 0.067894 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,617 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,618 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,618 INFO datasources.FileSourceStrategy: Output Data Schema: struct<occupation: string>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,640 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 317.5 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,658 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:41431 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,662 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1027.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,663 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,664 INFO spark.SparkContext: Created broadcast 9 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,665 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,666 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,673 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.232.55:34299 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,680 INFO scheduler.DAGScheduler: Registering RDD 24 (collect at StringIndexer.scala:204) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,680 INFO scheduler.DAGScheduler: Got map stage job 6 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,680 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,680 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,681 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,681 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,691 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:41431 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,697 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.232.55:34299 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,706 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.4 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,708 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,708 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.232.55:34299 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,708 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,709 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,709 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,711 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.232.55:34299 in memory (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,711 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,712 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:41431 in memory (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,742 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:41431 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,763 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.232.55:34299 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,777 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:41431 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,794 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.232.55:34299 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,812 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.232.55:34299 in memory (size: 30.4 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,813 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:41431 in memory (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:39,975 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:42047 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,382 INFO executor.YarnCoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.232.55:35065\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,397 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,397 INFO resource.ResourceUtils: No custom resources configured for spark.executor.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,398 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,539 INFO executor.YarnCoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,543 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,700 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42047.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,700 INFO netty.NettyBlockTransferService: Server created on algo-1:42047\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,702 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,713 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 42047, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,727 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 42047, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,728 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 42047, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,728 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,752 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:36,752 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:38,083 INFO executor.YarnCoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 1623 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:39,719 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:39,734 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:39,825 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,079 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,109 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 3). 3604 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,294 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,294 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 4)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,296 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,301 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,303 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,304 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,307 INFO datasources.FileScanRDD: TID: 4 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,309 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,314 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,316 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,333 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,375 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,411 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 4). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,464 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,464 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 5)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,465 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,466 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:41,130 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:42047 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,394 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 3683 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,394 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,395 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (collect at StringIndexer.scala:204) finished in 3.713 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,395 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,395 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,395 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,395 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,418 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,419 INFO scheduler.DAGScheduler: Got job 7 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,419 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,419 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,419 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,420 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,424 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 22.0 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,426 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,427 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.232.55:34299 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,427 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,428 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[27] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,428 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,430 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 7) (algo-1, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,450 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:42047 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,608 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.232.55:37858\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,751 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 7) in 321 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,751 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,752 INFO scheduler.DAGScheduler: ResultStage 11 (collect at StringIndexer.scala:204) finished in 0.330 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,752 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,752 INFO cluster.YarnScheduler: Killing all running tasks in stage 11: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,756 INFO scheduler.DAGScheduler: Job 7 finished: collect at StringIndexer.scala:204, took 0.338000 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,859 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,859 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,859 INFO datasources.FileSourceStrategy: Output Data Schema: struct<relationship: string>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,879 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,891 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,892 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,893 INFO spark.SparkContext: Created broadcast 12 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,894 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,894 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,897 INFO scheduler.DAGScheduler: Registering RDD 31 (collect at StringIndexer.scala:204) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,898 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,898 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 12 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,898 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,898 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,899 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,907 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.4 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,909 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,909 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.232.55:34299 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,910 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,910 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[31] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,910 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,911 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,927 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:41431 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:43,946 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:41431 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:39,888 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:39,941 INFO client.TransportClientFactory: Successfully created connection to /10.0.232.55:34299 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:39,971 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:39,980 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 92 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:40,087 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:40,654 INFO datasources.FileScanRDD: TID: 6 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,119 INFO codegen.CodeGenerator: Code generated in 194.661355 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,121 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,128 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,131 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,189 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,714 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,756 INFO codegen.CodeGenerator: Code generated in 9.612239 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,778 INFO codegen.CodeGenerator: Code generated in 8.408829 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:41,797 INFO codegen.CodeGenerator: Code generated in 10.236759 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:42,964 INFO codegen.CodeGenerator: Code generated in 15.266848 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,106 INFO codegen.CodeGenerator: Code generated in 17.947722 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,390 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 6). 2162 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,433 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,069 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 158 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,069 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,070 INFO scheduler.DAGScheduler: ShuffleMapStage 12 (collect at StringIndexer.scala:204) finished in 0.170 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,070 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,070 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,070 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,071 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,095 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,096 INFO scheduler.DAGScheduler: Got job 9 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,096 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,096 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,097 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,098 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[34] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,102 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,110 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,111 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.232.55:34299 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,112 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,117 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[34] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,117 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,118 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (algo-2, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,129 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:42047 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,132 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.232.55:34299 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,147 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:41431 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,155 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.236.247:51752\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,177 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:41431 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,180 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.232.55:34299 in memory (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,194 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.232.55:34299 in memory (size: 30.4 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,195 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:42047 in memory (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,199 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 81 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,200 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,200 INFO scheduler.DAGScheduler: ResultStage 14 (collect at StringIndexer.scala:204) finished in 0.101 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,201 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,201 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,201 INFO scheduler.DAGScheduler: Job 9 finished: collect at StringIndexer.scala:204, took 0.105459 s\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,472 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,474 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,475 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,479 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,479 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.232.55:35065)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,482 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,483 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,483 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:39,509 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 5). 3537 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,914 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,914 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 8)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,915 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,916 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,925 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,928 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 11 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,929 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,933 INFO datasources.FileScanRDD: TID: 8 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,940 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,237 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.232.55:34299 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,237 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:42047 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,313 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,313 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,314 INFO datasources.FileSourceStrategy: Output Data Schema: struct<race: string>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,332 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,343 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,344 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,345 INFO spark.SparkContext: Created broadcast 15 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,346 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,346 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,351 INFO scheduler.DAGScheduler: Registering RDD 38 (collect at StringIndexer.scala:204) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,351 INFO scheduler.DAGScheduler: Got map stage job 10 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,351 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,351 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,352 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,352 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[38] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,363 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.4 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,364 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,364 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.232.55:34299 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,365 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,365 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[38] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,365 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,366 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 10) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,384 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:42047 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,404 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:42047 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,578 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 10) in 212 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,578 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,579 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at StringIndexer.scala:204) finished in 0.226 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,581 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,581 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,581 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,581 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,603 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,604 INFO scheduler.DAGScheduler: Got job 11 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,604 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,604 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,605 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,605 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[41] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,608 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,609 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,610 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.232.55:34299 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,610 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,610 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[41] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,611 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,612 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 11) (algo-1, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,630 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:42047 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,639 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.232.55:37858\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,677 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 11) in 64 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,677 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,678 INFO scheduler.DAGScheduler: ResultStage 17 (collect at StringIndexer.scala:204) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,679 INFO scheduler.DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,679 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,680 INFO scheduler.DAGScheduler: Job 11 finished: collect at StringIndexer.scala:204, took 0.075940 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,767 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,767 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,768 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,786 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 317.5 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,800 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1027.4 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,801 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,802 INFO spark.SparkContext: Created broadcast 18 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,803 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,803 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,807 INFO scheduler.DAGScheduler: Registering RDD 45 (collect at StringIndexer.scala:204) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,807 INFO scheduler.DAGScheduler: Got map stage job 12 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,808 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,808 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,808 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,809 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[45] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,820 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 19.4 KiB, free 1027.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,822 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.3 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,822 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.232.55:34299 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,823 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,824 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[45] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,824 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,825 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 12) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,836 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:41431 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,852 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:42047 in memory (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,853 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.232.55:34299 in memory (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,862 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:41431 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,863 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.232.55:34299 in memory (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,864 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:41431 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,878 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.232.55:34299 in memory (size: 30.4 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,882 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:41431 in memory (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,887 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:42047 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,890 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.232.55:34299 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,896 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:42047 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,896 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.232.55:34299 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,948 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 12) in 123 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,948 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,949 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (collect at StringIndexer.scala:204) finished in 0.139 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,949 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,949 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,949 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,949 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,970 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,970 INFO scheduler.DAGScheduler: Got job 13 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,970 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,970 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,971 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,971 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[48] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,973 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 22.0 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,976 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,977 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.232.55:34299 (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,977 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,978 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[48] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,978 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,979 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 13) (algo-2, executor 2, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,989 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:41431 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:44,994 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.236.247:51752\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,021 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 13) in 42 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,021 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,022 INFO scheduler.DAGScheduler: ResultStage 20 (collect at StringIndexer.scala:204) finished in 0.050 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,022 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,023 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,023 INFO scheduler.DAGScheduler: Job 13 finished: collect at StringIndexer.scala:204, took 0.053292 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,433 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,440 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,441 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,448 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,452 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,453 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,603 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,605 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.232.55:35065)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,629 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,658 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,659 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,703 INFO codegen.CodeGenerator: Code generated in 12.81961 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:43,747 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 7). 3697 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,369 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,369 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 10)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,370 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,371 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,382 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,385 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,386 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,394 INFO datasources.FileScanRDD: TID: 10 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,397 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,403 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,405 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,431 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,525 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,576 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 10). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,615 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,615 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 11)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,617 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,618 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,629 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,631 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,632 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,637 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,637 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.232.55:35065)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,640 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,112 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,113 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,113 INFO datasources.FileSourceStrategy: Output Data Schema: struct<native_country: string>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,128 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,136 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,137 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,138 INFO spark.SparkContext: Created broadcast 21 from collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,138 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,139 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,142 INFO scheduler.DAGScheduler: Registering RDD 52 (collect at StringIndexer.scala:204) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,142 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,142 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 21 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,142 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,142 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,142 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[52] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,150 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.4 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,151 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,152 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.232.55:34299 (size: 9.7 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,152 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,153 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[52] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,153 INFO cluster.YarnScheduler: Adding task set 21.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,154 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4908 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,169 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:42047 (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,199 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:42047 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,945 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,947 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:43,963 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,031 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,066 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 8). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,131 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,132 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 9)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,133 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,133 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,146 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,149 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 15 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,150 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,154 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,154 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.232.55:35065)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,168 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,169 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,169 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,196 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 9). 3516 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,828 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,828 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 12)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,829 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,830 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,835 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,849 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 19 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,850 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,854 INFO datasources.FileScanRDD: TID: 12 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,856 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,861 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,864 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,872 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,916 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,945 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 12). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,982 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,982 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 13)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,983 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,984 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,303 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 149 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,303 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,304 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (collect at StringIndexer.scala:204) finished in 0.161 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,305 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,305 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,305 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,305 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,340 INFO spark.SparkContext: Starting job: collect at StringIndexer.scala:204\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,341 INFO scheduler.DAGScheduler: Got job 15 (collect at StringIndexer.scala:204) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,341 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (collect at StringIndexer.scala:204)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,341 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,341 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,343 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[55] at collect at StringIndexer.scala:204), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,346 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 22.0 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,347 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 1027.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,347 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.232.55:34299 (size: 10.9 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,348 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,349 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[55] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,349 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,350 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4706 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,363 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:42047 (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,370 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.232.55:37858\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,406 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 15) in 56 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,406 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,407 INFO scheduler.DAGScheduler: ResultStage 23 (collect at StringIndexer.scala:204) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,408 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,408 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,409 INFO scheduler.DAGScheduler: Job 15 finished: collect at StringIndexer.scala:204, took 0.069058 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,613 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-2:41431 in memory (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,621 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.232.55:34299 in memory (size: 30.4 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,627 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.232.55:34299 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,631 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:42047 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,636 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.232.55:34299 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,643 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:42047 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,671 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-2:41431 in memory (size: 10.9 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,690 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.232.55:34299 in memory (size: 10.9 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,717 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-2:41431 in memory (size: 9.7 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:45,718 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.232.55:34299 in memory (size: 9.7 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,642 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (593.0 B) non-empty blocks including 1 (593.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,642 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:44,674 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 11). 3524 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,157 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,157 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 14)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,158 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,160 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,167 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,170 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,170 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 19.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,177 INFO datasources.FileScanRDD: TID: 14 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,188 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,197 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,200 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,214 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,261 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,299 INFO executor.Executor: Finished task 0.0 in stage 21.0 (TID 14). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stderr] 2024-11-05 14:41:45,353 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,038 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,130 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,130 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,130 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: int, workclass: string, fnlwgt: int, education: string, education_num: int ... 13 more fields>\u001b[0m\n",
      "\u001b[34m11-05 14:41 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1730817666125_0001.inprogress\u001b[0m\n",
      "\u001b[34m11-05 14:41 root         INFO     copying /tmp/spark-events/application_1730817666125_0001.inprogress to /opt/ml/processing/spark-events/application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,320 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:42047 in memory (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,321 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.232.55:34299 in memory (size: 30.4 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,815 INFO codegen.CodeGenerator: Code generated in 387.300237 ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,819 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 317.5 KiB, free 1028.2 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,828 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1028.1 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,829 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,830 INFO spark.SparkContext: Created broadcast 24 from toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,830 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,831 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,938 INFO spark.SparkContext: Starting job: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,939 INFO scheduler.DAGScheduler: Got job 16 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,939 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,939 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,940 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,940 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[59] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,947 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 230.1 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,952 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 61.0 KiB, free 1027.9 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,953 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.232.55:34299 (size: 61.0 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,954 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,954 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[59] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,954 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,956 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 16) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 5080 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:46,967 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-2:41431 (size: 61.0 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:47,769 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-2:41431 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,168 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 16) in 1213 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,169 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,169 INFO scheduler.DAGScheduler: ResultStage 24 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108) finished in 1.228 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,170 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,170 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,170 INFO scheduler.DAGScheduler: Job 16 finished: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:108, took 1.231518 s\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,988 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 10.9 KiB, fre[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:29.984+0000: [GC (Allocation Failure) [PSYoungGen: 56320K->6630K(65536K)] 56320K->6638K(216064K), 0.0062959 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:30.469+0000: [GC (Allocation Failure) [PSYoungGen: 62950K->7358K(65536K)] 62958K->7374K(216064K), 0.0098656 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:30.762+0000: [GC (Metadata GC Threshold) [PSYoungGen: 49950K->7498K(65536K)] 49966K->7522K(216064K), 0.0059942 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:30.768+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 7498K->0K(65536K)] [ParOldGen: 24K->7231K(91136K)] 7522K->7231K(156672K), [Metaspace: 20396K->20396K(1067008K)], 0.0246399 secs] [Times: user=0.05 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:31.235+0000: [GC (Allocation Failure) [PSYoungGen: 56320K->4332K(89088K)] 63551K->11572K(180224K), 0.0047420 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:31.628+0000: [GC (Allocation Failure) [PSYoungGen: 88812K->6623K(117248K)] 96052K->13863K(208384K), 0.0071961 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:31.816+0000: [GC (Metadata GC Threshold) [PSYoungGen: 49606K->5470K(136704K)] 56845K->12717K(227840K), 0.0074482 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:31.823+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 5470K->0K(136704K)] [ParOldGen: 7247K->10197K(135680K)] 12717K->10197K(272384K), [Metaspace: 33934K->33931K(1079296K)], 0.0298760 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:33.562+0000: [GC (Allocation Failure) [PSYoungGen: 128000K->8691K(136704K)] 138197K->28040K(272384K), 0.0186481 secs] [Times: user=0.03 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:33.942+0000: [GC (Metadata GC Threshold) [PSYoungGen: 69875K->11695K(200192K)] 89223K->31052K(335872K), 0.0172796 secs] [Times: user=0.06 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:33.960+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 11695K->0K(200192K)] [ParOldGen: 19356K->19089K(189440K)] 31052K->19089K(389632K), [Metaspace: 56008K->56008K(1101824K)], 0.0956242 secs] [Times: user=0.25 sys=0.00, real=0.09 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:35.735+0000: [GC (Allocation Failure) [PSYoungGen: 186368K->13070K(201216K)] 205457K->32168K(390656K), 0.0143695 secs] [Times: user=0.03 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:38.840+0000: [GC (Allocation Failure) [PSYoungGen: 199438K->16365K(245248K)] 218536K->37439K(434688K), 0.0192085 secs] [Times: user=0.03 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stdout] 2024-11-05T14:41:47.905+0000: [GC (Allocation Failure) [PSYoungGen: 216472K->15761K(247808K)] 237546K->36850K(4e 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,990 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,991 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 22.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,994 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,994 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.232.55:35065)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,996 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,997 INFO storage.ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:44,997 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:45,019 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 13). 3428 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:46,958 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:46,959 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 16)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:46,960 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:46,961 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:46,966 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 61.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:46,968 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:46,969 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 230.1 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,514 INFO codegen.CodeGenerator: Code generated in 320.799084 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,693 INFO codegen.CodeGenerator: Code generated in 99.911106 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,721 INFO codegen.CodeGenerator: Code generated in 9.54716 ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,317 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-2:41431 in memory (size: 61.0 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,317 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.232.55:34299 in memory (size: 61.0 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,681 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,681 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:48,681 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: int, workclass: string, fnlwgt: int, education: string, education_num: int ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,013 INFO codegen.CodeGenerator: Code generated in 247.168542 ms\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,017 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 317.5 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,030 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 1027.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,031 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.232.55:34299 (size: 30.4 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,032 INFO spark.SparkContext: Created broadcast 26 from toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,033 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,034 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,062 INFO spark.SparkContext: Starting job: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,063 INFO scheduler.DAGScheduler: Got job 17 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,064 INFO scheduler.DAGScheduler: Final stage: ResultStage 25 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,064 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,064 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,065 INFO scheduler.DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[63] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,073 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 230.1 KiB, free 1027.6 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,076 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 61.0 KiB, free 1027.5 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,077 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.232.55:34299 (size: 61.0 KiB, free: 1028.7 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,078 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1479\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,079 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[63] at toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,079 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,081 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (algo-2, executor 2, partition 0, PROCESS_LOCAL, 5080 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,092 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-2:41431 (size: 61.0 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,315 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-2:41431 (size: 30.4 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,506 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 425 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,506 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,507 INFO scheduler.DAGScheduler: ResultStage 25 (toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110) finished in 0.438 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,508 INFO scheduler.DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,508 INFO cluster.YarnScheduler: Killing all running tasks in stage 25: Stage finished\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,508 INFO scheduler.DAGScheduler: Job 17 finished: toPandas at /opt/ml/processing/input/code/pyspark_preprocessing.py:110, took 0.445684 s\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,562 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-2:41431 in memory (size: 61.0 KiB, free: 6.3 GiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,572 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.232.55:34299 in memory (size: 61.0 KiB, free: 1028.8 MiB)\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,771 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,779 INFO server.AbstractConnector: Stopped Spark@a715675{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,782 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.232.55:4040\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,786 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,798 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,798 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,803 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,825 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,854 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,854 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,867 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,881 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,884 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000003 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,884 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,895 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,895 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,896 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-64382f3d-cb79-45cd-a344-b3c85b4d0b0e\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,902 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-64382f3d-cb79-45cd-a344-b3c85b4d0b0e/pyspark-e4ab256d-3cd4-4ad9-a2de-d88f96e6d636\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,906 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1730817666125_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,907 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,907 INFO rmapp.RMAppImpl: Updating application application_1730817666125_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,907 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-347caa8a-1c7d-4590-9b85-a245c3c7a7cb\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,907 INFO rmapp.RMAppImpl: application_1730817666125_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,907 INFO recovery.RMStateStore: Updating info for app: application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,908 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,908 INFO rmapp.RMAppImpl: application_1730817666125_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,912 INFO launcher.ContainerLaunch: Container container_1730817666125_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,916 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,916 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,917 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,922 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,923 INFO launcher.ContainerCleanup: Cleaning up container container_1730817666125_0001_01_000002\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,925 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,936 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000002\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,939 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,940 INFO application.ApplicationImpl: Removing container_1730817666125_0001_01_000002 from application application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,940 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1730817666125_0001_01_000002\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,940 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,942 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,943 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,951 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002/launch_container.sh\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,951 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002/launch_container.sh]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,951 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002/container_tokens\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,952 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002/container_tokens]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,952 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002/sysfs\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:49,952 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000002/sysfs]\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,009 INFO resourcemanager.ApplicationMasterService: application_1730817666125_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_173[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:34.916+0000: [GC (Allocation Failure) [PSYoungGen: 56320K->7022K(65536K)] 56320K->7030K(216064K), 0.0069876 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:35.434+0000: [GC (Allocation Failure) [PSYoungGen: 63342K->7549K(65536K)] 63350K->7565K(216064K), 0.0084091 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:35.695+0000: [GC (Metadata GC Threshold) [PSYoungGen: 47717K->7174K(65536K)] 47733K->7198K(216064K), 0.0054312 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:35.700+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 7174K->0K(65536K)] [ParOldGen: 24K->6981K(92160K)] 7198K->6981K(157696K), [Metaspace: 20399K->20399K(1067008K)], 0.0255334 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:36.099+0000: [GC (Allocation Failure) [PSYoungGen: 56320K->4483K(88576K)] 63301K->11472K(180736K), 0.0049624 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:36.414+0000: [GC (Allocation Failure) [PSYoungGen: 88451K->6230K(117248K)] 95440K->13227K(209408K), 0.0062337 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:36.610+0000: [GC (Metadata GC Threshold) [PSYoungGen: 53277K->5599K(135168K)] 60275K->12597K(227328K), 0.0067273 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:36.617+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 5599K->0K(135168K)] [ParOldGen: 6997K->10351K(141312K)] 12597K->10351K(276480K), [Metaspace: 33926K->33923K(1079296K)], 0.0285068 secs] [Times: user=0.06 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:40.033+0000: [GC (Allocation Failure) [PSYoungGen: 126976K->8180K(135168K)] 137327K->28603K(276480K), 0.0180482 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:40.344+0000: [GC (Metadata GC Threshold) [PSYoungGen: 57532K->11461K(192512K)] 77955K->31884K(333824K), 0.0127801 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:40.357+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 11461K->0K(192512K)] [ParOldGen: 20422K->18843K(195072K)] 31884K->18843K(387584K), [Metaspace: 56004K->56004K(1101824K)], 0.0994895 secs] [Times: user=0.26 sys=0.00, real=0.10 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:42.056+0000: [GC (Allocation Failure) [PSYoungGen: 178688K->12433K(193536K)] 197531K->31285K(388608K), 0.0134000 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] 2024-11-05T14:41:43.337+0000: [GC (Allocation Failure) [PSYoungGen: 191121K->15513K(239616K)] 209973K->34372K(434688K), 0.0155060 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout] Heap\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,865 INFO launcher.ContainerLaunch: Container container_1730817666125_0001_01_000003 succeeded \u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,874 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,875 INFO launcher.ContainerCleanup: Cleaning up container container_1730817666125_0001_01_000003\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,877 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000003\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,878 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000003\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,880 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,880 INFO application.ApplicationImpl: Removing container_1730817666125_0001_01_000003 from application application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,882 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1730817666125_0001_01_000003\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,883 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,898 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000003/launch_container.sh\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,898 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000003/launch_container.sh]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,898 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000003/container_tokens\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,898 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000003/container_tokens]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,898 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000003/sysfs\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:49,898 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000003/sysfs]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,740 INFO datasources.FileScanRDD: TID: 16 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,760 INFO codegen.CodeGenerator: Code generated in 13.968344 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,762 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,768 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,770 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,779 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 441.4 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,828 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,839 INFO codegen.CodeGenerator: Code generated in 6.828722 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:47,869 INFO codegen.CodeGenerator: Code generated in 9.89827 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:48,164 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 16). 217654 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,083 INFO executor.YarnCoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,084 INFO executor.Executor: Running task 0.0 in stage 25.0 (TID 17)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,086 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27 with 1 pieces (estimated total size 4.0 MiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,091 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 61.0 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,093 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,095 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 230.1 KiB, free 6.3 GiB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,255 INFO codegen.CodeGenerator: Code generated in 138.099722 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,297 INFO datasources.FileScanRDD: TID: 17 - Reading current file: path: s3://labdatabucket-us-west-2-226013955/data/input/spark_adult_data.csv, range: 0-118405, partition values: [empty row], isDataPresent: false, eTag: null\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,365 INFO launcher.ContainerLaunch: Container container_1730817666125_0001_01_000001 succeeded \u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,366 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,366 INFO launcher.ContainerCleanup: Cleaning up container container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,366 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,367 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,368 INFO container.ContainerImpl: Container container_1730817666125_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,368 INFO application.ApplicationImpl: Removing container_1730817666125_0001_01_000001 from application application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,368 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,368 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,381 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001/launch_container.sh\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,381 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001/launch_container.sh]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,381 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001/container_tokens\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,381 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001/container_tokens]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,381 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001/sysfs\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,381 WARN nodemanager.DefaultContainerExecutor: delete returned false for path: [/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001/container_1730817666125_0001_01_000001/sysfs]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,385 INFO ipc.Server: Auth successful for appattempt_1730817666125_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,389 INFO containermanager.ContainerManagerImpl: Stopping container with container Id: container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:50,389 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.232.55#011OPERATION=Stop Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000001\u001b[0m\n",
      "\u001b[34m11-05 14:41 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,370 INFO rmcontainer.RMContainerImpl: container_1730817666125_0001_01_000001 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,370 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,370 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1730817666125_0001#011CONTAINERID=container_1730817666125_0001_01_000001#011RESOURCE=<memory:896, max memory:15892, vCores:1, max vCores:4>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,371 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,373 INFO attempt.RMAppAttemptImpl: appattempt_1730817666125_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,375 INFO rmapp.RMAppImpl: application_1730817666125_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,375 INFO capacity.CapacityScheduler: Application Attempt appattempt_1730817666125_0001_000001 is done. finalState=FINISHED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,376 INFO scheduler.AppSchedulingInfo: Application application_1730817666125_0001 requests cleared\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,376 INFO amlauncher.AMLauncher: Cleaning master appattempt_1730817666125_0001_000001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,377 INFO capacity.LeafQueue: Application removed - appId: application_1730817666125_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,377 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=Application Finished - Succeeded#011TARGET=RMAppManager#011RESULT=SUCCESS#011APPID=application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,377 INFO capacity.ParentQueue: Application removed - appId: application_1730817666125_0001 user: root leaf-queue of parent: root #applications: 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,379 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1730817666125_0001,name=PySparkApp,user=root,queue=default,state=FINISHED,trackingUrl=http://algo-1:8088/proxy/application_1730817666125_0001/,appMasterHost=10.0.236.247,submitTime=1730817679965,startTime=1730817680031,launchTime=1730817680620,finishTime=1730817709907,finalStatus=SUCCEEDED,memorySeconds=618496,vcoreSeconds=72,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=SPARK,resourceSeconds=618496 MB-seconds\\, 72 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\\, 0 vcore-seconds,applicationTags=\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,946 INFO application.ApplicationImpl: Application application_1730817666125_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,946 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,946 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,947 INFO application.ApplicationImpl: Application application_1730817666125_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:50,947 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1730817666125_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:51,371 INFO nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1730817666125_0001_01_000001]\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:51,374 INFO application.ApplicationImpl: Application application_1730817666125_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:51,374 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:51,374 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:51,375 INFO application.ApplicationImpl: Application application_1730817666125_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:51,375 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1730817666125_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:53,722 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 replica FinalizedReplica, blk_1073741825_1001, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:53,724 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 replica FinalizedReplica, blk_1073741826_1002, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:53,724 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 replica FinalizedReplica, blk_1073741827_1003, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:53,724 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 replica FinalizedReplica, blk_1073741828_1004, FINALIZED\n",
      "  getNumBytes()     = 46002121\n",
      "  getBytesOnDisk()  = 46002121\n",
      "  getVisibleLength()= 46002121\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:53,724 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 replica FinalizedReplica, blk_1073741829_1005, FINALIZED\n",
      "  getNumBytes()     = 889814\n",
      "  getBytesOnDisk()  = 889814\n",
      "  getVisibleLength()= 889814\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:53,725 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 replica FinalizedReplica, blk_1073741830_1006, FINALIZED\n",
      "  getNumBytes()     = 41587\n",
      "  getBytesOnDisk()  = 41587\n",
      "  getVisibleLength()= 41587\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:53,725 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 replica FinalizedReplica, blk_1073741831_1007, FINALIZED\n",
      "  getNumBytes()     = 266122\n",
      "  getBytesOnDisk()  = 266122\n",
      "  getVisibleLength()= 266122\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:54,124 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741825_1001 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:54,530 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741826_1002 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:54,936 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741827_1003 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:54,940 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741828_1004 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:54,940 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741829_1005 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:54,940 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741830_1006 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[34m2024-11-05 14:41:54,940 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741831_1007 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,882 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 replica FinalizedReplica, blk_1073741825_1001, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,884 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 replica FinalizedReplica, blk_1073741826_1002, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,884 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 replica FinalizedReplica, blk_1073741827_1003, FINALIZED\n",
      "  getNumBytes()     = 134217728\n",
      "  getBytesOnDisk()  = 134217728\n",
      "  getVisibleLength()= 134217728\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,884 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 replica FinalizedReplica, blk_1073741828_1004, FINALIZED\n",
      "  getNumBytes()     = 46002121\n",
      "  getBytesOnDisk()  = 46002121\n",
      "  getVisibleLength()= 46002121\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,884 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 replica FinalizedReplica, blk_1073741829_1005, FINALIZED\n",
      "  getNumBytes()     = 889814\n",
      "  getBytesOnDisk()  = 889814\n",
      "  getVisibleLength()= 889814\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,884 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 replica FinalizedReplica, blk_1073741830_1006, FINALIZED\n",
      "  getNumBytes()     = 41587\n",
      "  getBytesOnDisk()  = 41587\n",
      "  getVisibleLength()= 41587\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,885 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 replica FinalizedReplica, blk_1073741831_1007, FINALIZED\n",
      "  getNumBytes()     = 266122\n",
      "  getBytesOnDisk()  = 266122\n",
      "  getVisibleLength()= 266122\n",
      "  getVolume()       = /opt/amazon/hadoop/hdfs/datanode\n",
      "  getBlockURI()     = file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,894 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741825_1001 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,905 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741826_1002 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,914 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741827_1003 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,917 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741828_1004 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,917 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741829_1005 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,917 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741830_1006 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[35m2024-11-05 14:41:55,918 INFO impl.FsDatasetAsyncDiskService: Deleted BP-1770039169-10.0.232.55-1730817657400 blk_1073741831_1007 URI file:/opt/amazon/hadoop/hdfs/datanode/current/BP-1770039169-10.0.232.55-1730817657400/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[34m2024-11-05 14:42:05,718 INFO BlockStateChange: BLOCK* processReport 0xcff2e0d057100d9b: from storage DS-d72ca929-dc30-49cf-ae35-f6a0dbd139ed node DatanodeRegistration(10.0.232.55:9866, datanodeUuid=33c80061-107d-47c3-9915-39f3f0671658, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-8a47e930-446c-46f6-90d1-7b3cdbe0477b;nsid=681504235;c=1730817657400), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m2024-11-05 14:42:05,718 INFO datanode.DataNode: Successfully sent block report 0xcff2e0d057100d9b,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m2024-11-05 14:42:05,718 INFO datanode.DataNode: Got finalize command for block pool BP-1770039169-10.0.232.55-1730817657400\u001b[0m\n",
      "\u001b[34m11-05 14:42 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1730817666125_0001\u001b[0m\n",
      "\u001b[34m11-05 14:42 root         INFO     copying /tmp/spark-events/application_1730817666125_0001 to /opt/ml/processing/spark-events/application_1730817666125_0001\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:06,401 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.236.247\"; destination host is: \"algo-1\":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000002/stdout]  PSY\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:07,880 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.236.247\"; destination host is: \"algo-1\":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:791)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:750)\u001b[0m\n",
      "\u001b[35mCaused by: java.io.EOFException\u001b[0m\n",
      "\u001b[35m#011at java.io.DataInputStream.readInt(DataInputStream.java:392)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)\u001b[0m\n",
      "\u001b[35m11-05 14:42 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe228404130>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m11-05 14:42 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe228404310>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:11,880 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:12,881 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m11-05 14:42 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe2284046a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:13,881 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:14,882 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:15,883 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:16,883 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m11-05 14:42 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe2284048b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:17,884 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:18,884 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:19,885 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:20,886 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:20,887 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.net.ConnectException: Call From algo-2/10.0.236.247 to algo-1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy16.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:168)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:517)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:648)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:750)\u001b[0m\n",
      "\u001b[35mCaused by: java.net.ConnectException: Connection refused\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1435)\u001b[0m\n",
      "\u001b[35m#011... 9 more\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:22,889 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:23,889 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m2024-11-05 14:42:24,890 INFO ipc.Client: Retrying connect to server: algo-1/10.0.232.55:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m11-05 14:42 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe228404ac0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m11-05 14:42 smspark-submit INFO     primary is down, worker now exiting\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1730817666125_0001/container_1730817666125_0001_01_000003/stderr] 2024-11-05 14:41:49,301 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26 with 1 pieces (estimated total size 4.0 M\u001b[0m\n",
      "\n",
      "Spark Processing Job Completed.\n"
     ]
    }
   ],
   "source": [
    "#processing-job\n",
    "import os\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "# Amazon S3 path prefix\n",
    "input_raw_data_prefix = \"data/input\"\n",
    "output_preprocessed_data_prefix = \"data/output\"\n",
    "logs_prefix = \"logs\"\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor.run(\n",
    "    submit_app=\"pyspark_preprocessing.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", \n",
    "                         source=\"/opt/ml/processing/train\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"train\")),\n",
    "        ProcessingOutput(output_name=\"validation_data\", \n",
    "                         source=\"/opt/ml/processing/validation\",\n",
    "                         destination=\"s3://\" + os.path.join(bucket, output_preprocessed_data_prefix, \"validation\")),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_raw_data_prefix,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", output_preprocessed_data_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, logs_prefix),\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"Spark Processing Job Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Validate the data processing results\n",
    "\n",
    "Validate the output of the data processing job that you ran by reviewing the first five rows of the train and validation output datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://labdatabucket-us-west-2-226013955/data/output/train/\n",
      "\"(84,[3,12,22,34,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,165361.0,6.0,40.0])\"\n",
      "\"(84,[0,10,22,32,42,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,28544.0,7.0,20.0])\"\n",
      "\"(84,[0,10,22,32,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,40299.0,7.0,25.0])\"\n",
      "\"(84,[0,10,22,37,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,61838.0,7.0,40.0])\"\n",
      "\"(84,[0,12,22,32,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,190941.0,6.0,20.0])\"\n"
     ]
    }
   ],
   "source": [
    "#view-train-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/train/train_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://labdatabucket-us-west-2-226013955/data/output/validation/\n",
      "\"(84,[3,10,22,34,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,143331.0,7.0,40.0])\"\n",
      "\"(84,[3,6,22,34,43,46,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,240183.0,9.0,45.0])\"\n",
      "\"(84,[0,10,22,32,43,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,170194.0,7.0,25.0])\"\n",
      "\"(84,[0,6,22,30,42,46,50,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,366154.0,9.0,30.0])\"\n",
      "\"(84,[3,7,22,34,43,47,51,78,79,80,83],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,19.0,50626.0,10.0,20.0])\"\n"
     ]
    }
   ],
   "source": [
    "#view-validation-dataset\n",
    "print(\"Top 5 rows from s3://{}/{}/validation/\".format(bucket, output_preprocessed_data_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$output_preprocessed_data_prefix/validation/validation_features.csv - | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You have used SageMaker Processing to successfully create a Spark processing job using the SageMaker Python SDK and run a processing job.\n",
    "\n",
    "The next task of the lab focuses on data processing with SageMaker Processing and the built-in scikit-learn container.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 3: Perform data processing with SageMaker Processing and the built-in scikit-learn container**."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
